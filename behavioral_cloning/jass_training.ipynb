{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d972f000-fd6c-4778-8b11-1a86415a4793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "train_card_policy_bc.py\n",
    "\n",
    "Trains a card prediction policy using Behavioral Cloning (BC)\n",
    "based on data parsed into an HDF5 file.\n",
    "\n",
    "Assumes the HDF5 file contains 'state_bits' (bool, N x 929) and\n",
    "'action_card_bits' (bool, N x 13) datasets, as generated by the parser.\n",
    "Converts action_card_bits to action_index (int, N) during loading.\n",
    "\n",
    "Uses the standard imitation.algorithms.bc.BC trainer.\n",
    "Requires compatible versions of torch, gymnasium, stable-baselines3, and imitation.\n",
    "\"\"\"\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "# stable_baselines3 is implicitly used by imitation for policy structure\n",
    "import stable_baselines3 as sb3\n",
    "# Import core BC algorithm and data types\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import types\n",
    "# No need for TrainingMetrics or util imports here when using default loss_calculator\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import torch as th # Use th alias for torch conventions\n",
    "import traceback # To print full error tracebacks\n",
    "from typing import Tuple, Optional\n",
    "from sklearn.model_selection import train_test_split # Corrected import\n",
    "\n",
    "\n",
    "# --- Logging Configuration ---\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "HDF5_FILE_PATH = r'../../../Training_Data/jass.hdf5'  # <--- Path to the HDF5 file from your parser\n",
    "POLICY_SAVE_PATH = 'jass_bc_card_policy_591' # Model save path\n",
    "\n",
    "# --- Data Keys (Must match your parser's output HDF5 structure) ---\n",
    "OBS_KEY = 'state_bits'          # N x 929 boolean array\n",
    "ACTION_BITS_KEY = 'action_card_bits' # N x 13 boolean array (the played card's representation)\n",
    "\n",
    "# --- Model Dimensions (Based on your parser) ---\n",
    "INPUT_DIM = 929 # From your parser's STATE_BITS calculation\n",
    "CARD_ACTION_DIM = 9 # 9 possible card positions in hand (index 0-8)\n",
    "\n",
    "# --- Constants from Parser (Needed to interpret state_bits) ---\n",
    "CARD_BITS = 13 # Bits used to represent a single card in the parser\n",
    "NUM_CARDS_HISTORY = 32 # Number of history card slots in state (32 * 13 bits)\n",
    "NUM_CARDS_TABLE = 3  # Number of table card slots in state (3 * 13 bits) - matches parser code, not comment\n",
    "NUM_CARDS_HAND = 9   # Number of hand card slots in state (9 * 13 bits) - matches CARD_ACTION_DIM conceptually\n",
    "\n",
    "# Calculate the starting index of the player's hand bits within the state_bits vector\n",
    "# Order from parser: History, Table, Hand, Shown, Trump\n",
    "HAND_START_BIT_INDEX = (NUM_CARDS_HISTORY * CARD_BITS) + (NUM_CARDS_TABLE * CARD_BITS)\n",
    "# 32 * 13 = 416\n",
    "# 3 * 13  = 39\n",
    "# HAND_START_BIT_INDEX = 416 + 39 = 455. The hand bits are from index 455 up to 455 + (9*13) = 455 + 117 = 572.\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "VALIDATION_SPLIT_SIZE = 0.15 # Use 15% of data for validation (optional for BC eval)\n",
    "RANDOM_SEED = 42 # For reproducible train/test splits\n",
    "BC_BATCH_SIZE = 32\n",
    "BC_LEARNING_RATE = 3e-4\n",
    "BC_N_EPOCHS = 10 # Number of passes over the training data\n",
    "\n",
    "device = th.device(\"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# --- 1. Load Data Function (Kept Action Conversion) ---\n",
    "def load_bc_data_from_hdf5(filepath: str) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Loads observations and converts action card bits (N x 13 bool) to action indices (N x 1 int, 0-8)\n",
    "    by finding the index of the played card within the hand representation in the state.\n",
    "    Loads data from all groups in the HDF5 file.\n",
    "    \"\"\"\n",
    "    all_obs = []\n",
    "    all_action_indices = [] # Store the derived action indices (0-8)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        logging.error(f\"HDF5 file not found at: {filepath}\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            # Load data from a subset of groups for speed if needed, or all\n",
    "            game_groups = list(f.keys()) # Load all groups\n",
    "            #game_groups = list(f.keys())[:2000] # Example: Load only first 2000 games for faster testing\n",
    "            logging.info(f\"Found {len(game_groups)} game groups in HDF5 file for BC.\")\n",
    "\n",
    "            if not game_groups:\n",
    "                logging.error(\"No game groups found in the HDF5 file.\")\n",
    "                return None, None\n",
    "\n",
    "            for i, group_name in enumerate(game_groups):\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    logging.info(f\"Loading and processing group {i+1}/{len(game_groups)} for BC: {group_name}\")\n",
    "                try:\n",
    "                    group = f[group_name]\n",
    "                    # --- Check required keys ---\n",
    "                    if OBS_KEY not in group:\n",
    "                        logging.warning(f\"BC Skipping group '{group_name}': Missing dataset '{OBS_KEY}'\")\n",
    "                        continue\n",
    "                    # We now expect 'action_card_bits' from the parser\n",
    "                    if ACTION_BITS_KEY not in group:\n",
    "                        logging.error(f\"BC CRITICAL: Skipping group '{group_name}': Missing dataset '{ACTION_BITS_KEY}'.\")\n",
    "                        logging.error(f\"Ensure your parser saves the 13-bit card representation as '{ACTION_BITS_KEY}'.\")\n",
    "                        continue # Skip group if critical action data is missing\n",
    "\n",
    "                    obs_group_data = group[OBS_KEY][:] # Shape (N, INPUT_DIM) bool -> N x 929\n",
    "                    actions_bits_group_data = group[ACTION_BITS_KEY][:] # Shape (N, CARD_BITS) bool -> N x 13\n",
    "\n",
    "                    # --- Basic Validation ---\n",
    "                    if obs_group_data.shape[0] != actions_bits_group_data.shape[0]:\n",
    "                        logging.warning(f\"BC Skipping group '{group_name}': Data length mismatch ({obs_group_data.shape[0]} vs {actions_bits_group_data.shape[0]}).\")\n",
    "                        continue\n",
    "                    if obs_group_data.shape[0] == 0:\n",
    "                         logging.info(f\"BC Skipping empty group '{group_name}'.\")\n",
    "                         continue\n",
    "                    if obs_group_data.shape[1] != INPUT_DIM:\n",
    "                        logging.warning(f\"BC Skipping group '{group_name}': Observation dimension mismatch (Expected {INPUT_DIM}, Got {obs_group_data.shape[1]}).\")\n",
    "                        continue\n",
    "                    # Validate the shape of the action bits data from the parser\n",
    "                    if actions_bits_group_data.shape[1] != CARD_BITS:\n",
    "                         logging.warning(f\"BC Skipping group '{group_name}': Action bits dimension mismatch (Expected {CARD_BITS}, Got {actions_bits_group_data.shape[1]}).\")\n",
    "                         continue\n",
    "\n",
    "                    # --- Convert Action Card Bits to Action Index (0-8) ---\n",
    "                    # Iterate through each state-action pair in the group\n",
    "                    group_derived_indices = []\n",
    "                    # Convert boolean arrays to integers (0 or 1) for easier comparison/processing\n",
    "                    # This conversion is safe as boolean True/False become 1/0\n",
    "                    obs_group_int = obs_group_data.astype(np.int8)\n",
    "                    actions_bits_group_int = actions_bits_group_data.astype(np.int8)\n",
    "\n",
    "                    for j in range(obs_group_int.shape[0]):\n",
    "                        state_vec_int = obs_group_int[j, :] # This sample's state (929 integers 0/1)\n",
    "                        played_card_bits_int = actions_bits_group_int[j, :] # This sample's action card bits (13 integers 0/1)\n",
    "\n",
    "                        # Extract the player's hand bits from the state vector\n",
    "                        # Hand bits start at HAND_START_BIT_INDEX and are NUM_CARDS_HAND * CARD_BITS long\n",
    "                        hand_bits_int = state_vec_int[HAND_START_BIT_INDEX : HAND_START_BIT_INDEX + NUM_CARDS_HAND * CARD_BITS]\n",
    "\n",
    "                        # Find the index of the played_card_bits within the hand_bits\n",
    "                        action_index = -1 # Default to not found\n",
    "                        for card_idx_in_hand in range(NUM_CARDS_HAND):\n",
    "                            # Extract the bits for the current card in the hand\n",
    "                            start_idx = card_idx_in_hand * CARD_BITS\n",
    "                            end_idx = start_idx + CARD_BITS\n",
    "                            current_hand_card_bits_int = hand_bits_int[start_idx : end_idx]\n",
    "\n",
    "                            # Compare the played card bits with the current hand card bits\n",
    "                            # Use np.array_equal for a robust comparison of the 13-element vectors\n",
    "                            if np.array_equal(played_card_bits_int, current_hand_card_bits_int):\n",
    "                                action_index = card_idx_in_hand # Found the index!\n",
    "                                break # Found the card in hand, its index is our action\n",
    "\n",
    "                        if action_index == -1:\n",
    "                            # This indicates a problem: the card recorded as played was not found in the state's hand\n",
    "                            # (e.g., due to parsing errors, inconsistent state/action logging)\n",
    "                            # We skip this sample as we cannot determine the correct action index (0-8)\n",
    "                            # from the player's perspective at that state.\n",
    "                            # You could log the bit sequences for debugging if this happens often.\n",
    "                            # logging.warning(f\"BC WARNING: Card played (bits: {played_card_bits_int.tolist()}) not found in hand (bits: {hand_bits_int.reshape(-1, CARD_BITS).tolist()}) for sample {j} in group '{group_name}'. Skipping this sample.\")\n",
    "                            continue # Skip to the next sample (j)\n",
    "\n",
    "                        # If index was found (0-8), append it\n",
    "                        group_derived_indices.append(action_index)\n",
    "\n",
    "                    # Only append data from this group if we successfully derived at least one index\n",
    "                    if group_derived_indices:\n",
    "                        # Convert boolean observations to float32 for the model\n",
    "                        all_obs.append(obs_group_data.astype(np.float32)) # Use original boolean data for obs -> float32\n",
    "                        # Append the derived action indices for the samples that were not skipped\n",
    "                        all_action_indices.append(np.array(group_derived_indices, dtype=np.int64)) # Ensure actions are int64\n",
    "                    else:\n",
    "                         logging.warning(f\"BC: No valid action indices derived for group '{group_name}'. Skipping entire group.\")\n",
    "\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"BC Error processing group '{group_name}': {e}\")\n",
    "                    traceback.print_exc() # Print traceback for detailed error\n",
    "                    continue # Skip this group on error\n",
    "\n",
    "            if not all_obs or not all_action_indices:\n",
    "                logging.error(\"BC: No valid data loaded from any group.\")\n",
    "                return None, None\n",
    "\n",
    "            # Concatenate data from all groups\n",
    "            logging.info(\"BC: Concatenating data from all groups...\")\n",
    "            final_obs = np.concatenate(all_obs, axis=0)\n",
    "            final_action_indices = np.concatenate(all_action_indices, axis=0)\n",
    "            logging.info(\"BC: Concatenation complete.\")\n",
    "            logging.info(f\"Final concatenated data shapes: obs={final_obs.shape}, actions={final_action_indices.shape}\")\n",
    "\n",
    "\n",
    "            # Final sanity check on action indices range (should be 0-8)\n",
    "            # np.max and np.min will error on empty arrays, check size first\n",
    "            if final_action_indices.size > 0:\n",
    "                if np.max(final_action_indices) >= CARD_ACTION_DIM or np.min(final_action_indices) < 0:\n",
    "                     logging.error(f\"BC CRITICAL: Derived action indices out of range (0-{CARD_ACTION_DIM-1}). Max: {np.max(final_action_indices)}, Min: {np.min(final_action_indices)}\")\n",
    "                     # This indicates a major issue in the derivation logic or source data\n",
    "                     # You might want to raise an error or filter these out explicitly if they exist\n",
    "                     pass # Allow continuation for now, but investigate data if this warning appears often\n",
    "            else:\n",
    "                 logging.warning(\"No action indices were derived after loading and filtering data.\")\n",
    "\n",
    "\n",
    "            return final_obs, final_action_indices\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"BC: Failed to load data from HDF5 file '{filepath}': {e}\")\n",
    "        traceback.print_exc() # Print traceback for detailed error\n",
    "        return None, None\n",
    "\n",
    "observations, card_action_indices = load_bc_data_from_hdf5(HDF5_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34ac2413-2223-4ec4-a4b8-62b1e0b3c111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 12:59:31 - INFO - --- Starting Card Policy Training (BC) ---\n",
      "2025-04-20 12:59:31 - INFO - Total valid data samples loaded for BC: 734009.\n",
      "2025-04-20 12:59:31 - INFO - Splitting data for BC (keeping 85% for training)...\n",
      "2025-04-20 12:59:32 - INFO - BC Training samples: 623907, Validation samples: 110102\n",
      "2025-04-20 12:59:35 - INFO - Starting BC training for 10 epochs (~19498 batches per epoch) totaling 194980 batches...\n",
      "  0%|          | 0/194980 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.0022  |\n",
      "|    entropy        | 2.2      |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 129      |\n",
      "|    loss           | 2.19     |\n",
      "|    neglogp        | 2.2      |\n",
      "|    prob_true_act  | 0.111    |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 511/194980 [00:06<40:48, 79.41batch/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 89\u001b[0m\n\u001b[1;32m     85\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting BC training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBC_N_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs (~\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_batches_per_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m batches per epoch) totaling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m batches...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_batches \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Using n_batches argument which is compatible with many imitation versions\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# Adjust log_interval based on the total number of batches\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     \u001b[43mbc_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_batches\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Log more frequently, e.g., 50 times per training run\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo training batches available. Skipping training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/imitation/algorithms/bc.py:495\u001b[0m, in \u001b[0;36mBC.train\u001b[0;34m(self, n_epochs, n_batches, on_epoch_end, on_batch_end, log_interval, log_rollouts_venv, log_rollouts_n_episodes, progress_bar, reset_tensorboard)\u001b[0m\n\u001b[1;32m    490\u001b[0m obs_tensor \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mmap_maybe_dict(\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: util\u001b[38;5;241m.\u001b[39msafe_to_tensor(x, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    492\u001b[0m     types\u001b[38;5;241m.\u001b[39mmaybe_unwrap_dictobs(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m    493\u001b[0m )\n\u001b[1;32m    494\u001b[0m acts \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39msafe_to_tensor(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macts\u001b[39m\u001b[38;5;124m\"\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 495\u001b[0m training_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_calculator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Renormalise the loss to be averaged over the whole\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# batch size instead of the minibatch size.\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# If there is an incomplete batch, its gradients will be\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;66;03m# smaller, which may be helpful for stability.\u001b[39;00m\n\u001b[1;32m    501\u001b[0m loss \u001b[38;5;241m=\u001b[39m training_metrics\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m*\u001b[39m minibatch_size \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/imitation/algorithms/bc.py:130\u001b[0m, in \u001b[0;36mBehaviorCloningLossCalculator.__call__\u001b[0;34m(self, policy, obs, acts)\u001b[0m\n\u001b[1;32m    126\u001b[0m acts \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39msafe_to_tensor(acts)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# policy.evaluate_actions's type signatures are incorrect.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# See https://github.com/DLR-RM/stable-baselines3/issues/1679\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m (_, log_prob, entropy) \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_actions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43macts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m prob_true_act \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mexp(log_prob)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    135\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m log_prob\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/stable_baselines3/common/policies.py:732\u001b[0m, in \u001b[0;36mActorCriticPolicy.evaluate_actions\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m    730\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs)\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[0;32m--> 732\u001b[0m     latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    734\u001b[0m     pi_features, vf_features \u001b[38;5;241m=\u001b[39m features\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/stable_baselines3/common/torch_layers.py:257\u001b[0m, in \u001b[0;36mMlpExtractor.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[th\u001b[38;5;241m.\u001b[39mTensor, th\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    253\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    :return: latent_policy, latent_value of the specified network.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m        If all layers are shared, then ``latent_policy == latent_value``\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_actor(features), \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/stable_baselines3/common/torch_layers.py:263\u001b[0m, in \u001b[0;36mMlpExtractor.forward_critic\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_critic\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:392\u001b[0m, in \u001b[0;36mTanh.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"--- Starting Card Policy Training (BC) ---\")\n",
    "\n",
    "    # --- Load Data ---\n",
    "    # We now expect the second return value to be the action indices (0-8)\n",
    "    if observations is None or card_action_indices is None:\n",
    "        logging.critical(\"Could not load data for BC training. Exiting.\")\n",
    "        exit()\n",
    "    # Ensure observations and action indices have the same number of samples after loading and filtering\n",
    "    if len(observations) != len(card_action_indices):\n",
    "         logging.critical(f\"Observation and action index counts mismatch after loading: {len(observations)} vs {len(card_action_indices)}. Exiting.\")\n",
    "         exit()\n",
    "    if len(observations) == 0:\n",
    "         logging.critical(\"No valid data samples loaded for BC training. Exiting.\")\n",
    "         exit()\n",
    "    logging.info(f\"Total valid data samples loaded for BC: {len(observations)}.\")\n",
    "\n",
    "\n",
    "    # --- Split Data (Optional for BC, but good practice) ---\n",
    "    logging.info(f\"Splitting data for BC (keeping {1-VALIDATION_SPLIT_SIZE:.0%} for training)...\")\n",
    "    # We primarily need the training split for BC demonstrations\n",
    "    # Ensure both obs and action_indices are split together\n",
    "    obs_train, obs_val, act_train_indices, act_val_indices = train_test_split(\n",
    "        observations, card_action_indices, # Use the derived indices here\n",
    "        test_size=VALIDATION_SPLIT_SIZE,\n",
    "        random_state=RANDOM_SEED,\n",
    "        shuffle=True,\n",
    "        # stratify=card_action_indices # Optional: Stratify to ensure action distribution is similar in train/val splits - requires all indices to be valid (0-8). Uncomment if needed and all data is valid.\n",
    "    )\n",
    "    logging.info(f\"BC Training samples: {len(obs_train)}, Validation samples: {len(obs_val)}\")\n",
    "    # Free up memory from the original full arrays and validation splits\n",
    "    del observations, card_action_indices, obs_val, act_val_indices\n",
    "\n",
    "\n",
    "    # --- Prepare Data & Spaces for Imitation Library ---\n",
    "    # The Transitions object holds NumPy arrays. The batching logic in bc.train\n",
    "    # will convert these to tensors and move them to the device using its internal\n",
    "    # mechanism (e.g., safe_to_tensor).\n",
    "    dummy_next_obs_array = np.zeros_like(obs_train)\n",
    "\n",
    "    train_demonstrations = types.Transitions(\n",
    "        obs=obs_train, # NumPy array\n",
    "        acts=act_train_indices, # NumPy array of integer indices (0-8)\n",
    "        infos=np.array([None] * len(obs_train), dtype=object), # Dummy infos (NumPy array)\n",
    "        next_obs=dummy_next_obs_array, # Dummy next_obs (NumPy array)\n",
    "        dones=np.array([False] * len(obs_train)), # Dummy dones (NumPy array)\n",
    "    )\n",
    "    # Free up memory from the arrays now held by the Transitions object\n",
    "    del obs_train, act_train_indices\n",
    "\n",
    "\n",
    "    observation_space = gym.spaces.Box(low=0, high=1, shape=(INPUT_DIM,), dtype=np.float32)\n",
    "    action_space = gym.spaces.Discrete(CARD_ACTION_DIM) # Keep Discrete(9) for action index\n",
    "\n",
    "\n",
    "    # --- Train Behavioral Cloning Model ---\n",
    "    rng = np.random.default_rng(RANDOM_SEED)\n",
    "    bc_trainer = bc.BC(\n",
    "        observation_space=observation_space,\n",
    "        action_space=action_space,\n",
    "        demonstrations=train_demonstrations, # Pass the Transitions object (with NumPy arrays)\n",
    "        batch_size=BC_BATCH_SIZE,\n",
    "        optimizer_kwargs=dict(lr=BC_LEARNING_RATE),\n",
    "        device=device, # Ensure the trainer/policy are created on the correct device\n",
    "        rng=rng,\n",
    "        # *** USING DEFAULT loss_calculator ***\n",
    "        # If you encounter the device RuntimeError again, it means the default\n",
    "        # loss_calculator in your imitation version has the bug.\n",
    "        # In that case, you would need to use the minimalist custom loss calculator\n",
    "        # code from the previous response, which should work if torch and numpy are available.\n",
    "        # loss_calculator=CorrectedBehaviorCloningLossCalculator() # Uncomment if device bug persists with default\n",
    "    )\n",
    "\n",
    "    # Calculate n_batches based on the total number of training samples\n",
    "    n_batches_per_epoch = len(train_demonstrations.obs) // BC_BATCH_SIZE\n",
    "    if len(train_demonstrations.obs) % BC_BATCH_SIZE != 0:\n",
    "        n_batches_per_epoch += 1 # Account for the last partial batch\n",
    "\n",
    "    n_batches = n_batches_per_epoch * BC_N_EPOCHS\n",
    "    # Ensure n_batches is at least 1 if there's data\n",
    "    n_batches = max(1, n_batches) if len(train_demonstrations.obs) > 0 else 0\n",
    "\n",
    "\n",
    "    logging.info(f\"Starting BC training for {BC_N_EPOCHS} epochs (~{n_batches_per_epoch} batches per epoch) totaling {n_batches} batches...\")\n",
    "    if n_batches > 0:\n",
    "        # Using n_batches argument which is compatible with many imitation versions\n",
    "        # Adjust log_interval based on the total number of batches\n",
    "        bc_trainer.train(n_batches=n_batches, log_interval=max(1, n_batches // 50)) # Log more frequently, e.g., 50 times per training run\n",
    "    else:\n",
    "        logging.warning(\"No training batches available. Skipping training.\")\n",
    "\n",
    "\n",
    "    # --- Save Policy ---\n",
    "    if n_batches > 0: # Only attempt to save if training actually ran\n",
    "        try:\n",
    "            # The policy object is stored in bc_trainer.policy\n",
    "            # Save it using the underlying SB3 policy save method\n",
    "            # This saves an SB3-compatible policy, usually as a .zip file\n",
    "            bc_trainer.policy.save(f\"{POLICY_SAVE_PATH}.zip\")\n",
    "            logging.info(f\"Card prediction policy saved successfully to {POLICY_SAVE_PATH}.zip\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving BC policy: {e}\")\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        logging.warning(\"Skipping policy save because no training was performed.\")\n",
    "\n",
    "\n",
    "    logging.info(\"--- Card Policy Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad463ad-955d-496f-9b9e-48f09e18bb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
