{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d9275c-acce-4866-8312-7fc0fa3712b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "2025-04-22 10:33:32 - INFO - --- Script Start ---\n",
      "2025-04-22 10:33:32 - INFO - Using device: cpu\n",
      "2025-04-22 10:33:32 - INFO - Trainer initialized with parameters: {'validation_split_size': 0.15, 'random_seed': 42, 'bc_batch_size': 32, 'bc_learning_rate': 0.0003, 'bc_n_epochs': 10, 'device': device(type='cpu'), 'dataloader_num_workers': 0, 'dataloader_pin_memory': False}\n",
      "2025-04-22 10:33:32 - INFO - Using device for training: cpu\n",
      "2025-04-22 10:33:32 - INFO - Loading and preparing data...\n",
      "2025-04-22 10:33:40 - INFO - Found 35373 game groups total. Processing 2000 groups for locations.\n",
      "2025-04-22 10:33:40 - INFO - Skipping empty group 'S2500_00ac20ed3234029404a30fb0e06d7109'.\n",
      "2025-04-22 10:33:41 - INFO - Collecting locations from group 200/2000: S2500_019614d1a87dd918ff5af8ed06b5ca9a\n",
      "2025-04-22 10:33:42 - INFO - Skipping empty group 'S2500_0262972aa114bc043c893e8a1e99aad1'.\n",
      "2025-04-22 10:33:42 - INFO - Collecting locations from group 400/2000: S2500_030a5904c378b223faa093a8516c7450\n",
      "2025-04-22 10:33:43 - INFO - Collecting locations from group 600/2000: S2500_046210b509e17d7bd1a220972208274d\n",
      "2025-04-22 10:33:43 - INFO - Skipping empty group 'S2500_049292ad4c8103be55bcc178aa3a3620'.\n",
      "2025-04-22 10:33:44 - INFO - Collecting locations from group 800/2000: S2500_05d3c661958ef4f3d149fde68834efbd\n",
      "2025-04-22 10:33:45 - INFO - Skipping empty group 'S2500_06a7d30b3751fde4fc347acb627b58c8'.\n",
      "2025-04-22 10:33:45 - INFO - Collecting locations from group 1000/2000: S2500_0730d9d611061cc0a24fb83c62cdca11\n",
      "2025-04-22 10:33:46 - INFO - Collecting locations from group 1200/2000: S2500_08ad8d8504bacd89401f90cb6fbf2eef\n",
      "2025-04-22 10:33:47 - INFO - Collecting locations from group 1400/2000: S2500_0a441084ff94725af39997338af59276\n",
      "2025-04-22 10:33:48 - INFO - Collecting locations from group 1600/2000: S2500_0bda2727409f9e1a57fa33414b95cccf\n",
      "2025-04-22 10:33:49 - INFO - Collecting locations from group 1800/2000: S2500_0d4514b5e8240d108188abf7969d73fb\n",
      "2025-04-22 10:33:49 - INFO - Skipping empty group 'S2500_0d7d32a5ac1f42604f63ddb1e9d9dcea'.\n",
      "2025-04-22 10:33:50 - INFO - Skipping empty group 'S2500_0daa80e046d7f2c7d12a604560a360ef'.\n",
      "2025-04-22 10:33:50 - INFO - Collecting locations from group 2000/2000: S2500_0e83c87bdc2fce1894c8d078c1850b25\n",
      "2025-04-22 10:33:50 - INFO - Finished collecting locations for 2000 groups. Total potential samples found: 734009\n",
      "2025-04-22 10:33:50 - INFO - Total valid data sample locations collected: 734009.\n",
      "2025-04-22 10:33:50 - INFO - Splitting sample locations (keeping 85% for training)...\n",
      "2025-04-22 10:33:50 - INFO - Training locations: 623907, Validation locations: 110102\n",
      "2025-04-22 10:33:50 - INFO - Created training DataLoader with batch_size=32, num_workers=0\n",
      "2025-04-22 10:33:50 - INFO - Data preparation complete. Total training batches calculated: 194980\n",
      "2025-04-22 10:33:50 - INFO - Setting up BC trainer...\n",
      "2025-04-22 10:33:52 - INFO - BC trainer initialized successfully.\n",
      "2025-04-22 10:33:52 - INFO - Starting BC training for 10 epochs totaling 194980 batches...\n",
      "2025-04-22 10:33:52 - INFO - Logging training progress every 3899 batches.\n",
      "  0%|          | 0/194980 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.0022  |\n",
      "|    entropy        | 2.2      |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 129      |\n",
      "|    loss           | 2.2      |\n",
      "|    neglogp        | 2.2      |\n",
      "|    prob_true_act  | 0.111    |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 960/194980 [00:55<3:05:37, 17.42batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 579\u001b[0m\n\u001b[1;32m    575\u001b[0m      exit() \u001b[38;5;66;03m# Exit if trainer initialization failed\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;66;03m# 3. Run the training process\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;66;03m# The device mismatch error would likely happen inside trainer.train()\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    580\u001b[0m      logging\u001b[38;5;241m.\u001b[39mcritical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining process failed. Exiting.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    581\u001b[0m      exit() \u001b[38;5;66;03m# Exit if training encountered a critical error\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 495\u001b[0m, in \u001b[0;36mBCCardPolicyTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    493\u001b[0m log_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_batches \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m50\u001b[39m) \u001b[38;5;66;03m# Log ~50 times during training\u001b[39;00m\n\u001b[1;32m    494\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogging training progress every \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_interval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m batches.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbc_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBC training finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# Indicate success\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/imitation/algorithms/bc.py:483\u001b[0m, in \u001b[0;36mBC.train\u001b[0;34m(self, n_epochs, n_batches, on_epoch_end, on_batch_end, log_interval, log_rollouts_venv, log_rollouts_n_episodes, progress_bar, reset_tensorboard)\u001b[0m\n\u001b[1;32m    480\u001b[0m         on_batch_end()\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 483\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (\n\u001b[1;32m    484\u001b[0m     batch_num,\n\u001b[1;32m    485\u001b[0m     minibatch_size,\n\u001b[1;32m    486\u001b[0m     num_samples_so_far,\n\u001b[1;32m    487\u001b[0m ), batch \u001b[38;5;129;01min\u001b[39;00m batches_with_stats:\n\u001b[1;32m    488\u001b[0m     obs_tensor: Union[th\u001b[38;5;241m.\u001b[39mTensor, Dict[\u001b[38;5;28mstr\u001b[39m, th\u001b[38;5;241m.\u001b[39mTensor]]\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;66;03m# unwraps the observation if it's a dictobs and converts arrays to tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/imitation/algorithms/bc.py:164\u001b[0m, in \u001b[0;36menumerate_batches\u001b[0;34m(batch_it)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prepends batch stats before the batches of a batch iterator.\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m num_samples_so_far \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num_batches, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_it):\n\u001b[1;32m    165\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    166\u001b[0m     num_samples_so_far \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/imitation/algorithms/bc.py:64\u001b[0m, in \u001b[0;36mBatchIteratorWithEpochEndCallback.__iter__.<locals>.batch_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_num \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mislice(itertools\u001b[38;5;241m.\u001b[39mcount(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs):\n\u001b[1;32m     63\u001b[0m     some_batch_was_yielded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_loader:\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m batch\n\u001b[1;32m     66\u001b[0m         some_batch_was_yielded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/imitation/algorithms/base.py:212\u001b[0m, in \u001b[0;36m_WrappedDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[types\u001b[38;5;241m.\u001b[39mTransitionMapping]:\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Yields data from `self.data_loader`, checking `self.expected_batch_size`.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    Yields:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m            `self.expected_batch_size`.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_loader:\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_batch_size:\n\u001b[1;32m    214\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected batch size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = len(batch[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m])\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    217\u001b[0m             )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[1], line 256\u001b[0m, in \u001b[0;36mHDF5Dataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# Using [index:index+1] slicing reads a single row and keeps the dimension (shape (1, D))\u001b[39;00m\n\u001b[1;32m    255\u001b[0m obs_data_single \u001b[38;5;241m=\u001b[39m group[OBS_KEY][sample_index_in_group:sample_index_in_group\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# Shape (1, INPUT_DIM) bool\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m actions_bits_data_single \u001b[38;5;241m=\u001b[39m \u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[43mACTION_BITS_KEY\u001b[49m\u001b[43m]\u001b[49m[sample_index_in_group:sample_index_in_group\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# Shape (1, CARD_BITS) bool\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# --- Convert Action Card Bits (1, 13) bool to Action Index (scalar) int64 ---\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Convert boolean arrays to integers for easier comparison/processing\u001b[39;00m\n\u001b[1;32m    260\u001b[0m state_vec_int_single \u001b[38;5;241m=\u001b[39m obs_data_single\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint8)\u001b[38;5;241m.\u001b[39msqueeze(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# Shape (INPUT_DIM,) int8\u001b[39;00m\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/h5py/_hl/group.py:369\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Group(oid)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m otype \u001b[38;5;241m==\u001b[39m h5i\u001b[38;5;241m.\u001b[39mDATASET:\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43moid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreadonly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m otype \u001b[38;5;241m==\u001b[39m h5i\u001b[38;5;241m.\u001b[39mDATATYPE:\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m datatype\u001b[38;5;241m.\u001b[39mDatatype(oid)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/h5py/_hl/dataset.py:682\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, bind, readonly)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;124;03m    The active filters of the dataset.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filters\u001b[38;5;241m.\u001b[39mget_filters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dcpl)\n\u001b[0;32m--> 682\u001b[0m \u001b[38;5;129m@with_phil\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, bind, \u001b[38;5;241m*\u001b[39m, readonly\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    684\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Create a new Dataset object by binding to a low-level DatasetID.\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(bind, h5d\u001b[38;5;241m.\u001b[39mDatasetID):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "train_card_policy_bc.py\n",
    "\n",
    "Trains a card prediction policy using Behavioral Cloning (BC)\n",
    "based on data parsed into an HDF5 file.\n",
    "\n",
    "This version uses a class to organize the training process\n",
    "and leverages a custom HDF5 dataset with DataLoader for\n",
    "memory-efficient training on large datasets.\n",
    "\n",
    "Requires compatible versions of torch, gymnasium, stable-baselines3, imitation, and h5py.\n",
    "\"\"\"\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "# stable_baselines3 is implicitly used by imitation for policy structure\n",
    "import stable_baselines3 as sb3\n",
    "# Import core BC algorithm and data types\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import types\n",
    "# Need Dataset and DataLoader for custom data loading\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as data_utils # Use alias for DataLoader\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import torch as th # Use th alias for torch conventions\n",
    "import traceback # To print full error tracebacks\n",
    "from typing import Tuple, Optional, List, Dict, Any\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# --- Logging Configuration ---\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "HDF5_FILE_PATH = r'../../../../Training_Data/jass.hdf5' # <--- Path to the HDF5 file from your parser\n",
    "POLICY_SAVE_PATH = 'jass_bc_card_policy_591_organized' # Model save path\n",
    "\n",
    "# --- Data Keys (Must match your parser's output HDF5 structure) ---\n",
    "OBS_KEY = 'state_bits'         # N x 929 boolean array\n",
    "ACTION_BITS_KEY = 'action_card_bits' # N x 13 boolean array (the played card's representation)\n",
    "\n",
    "# --- Model Dimensions (Based on your parser) ---\n",
    "INPUT_DIM = 929 # From your parser's STATE_BITS calculation\n",
    "CARD_ACTION_DIM = 9 # 9 possible card positions in hand (index 0-8)\n",
    "\n",
    "# --- Constants from Parser (Needed to interpret state_bits) ---\n",
    "CARD_BITS = 13 # Bits used to represent a single card in the parser\n",
    "NUM_CARDS_HISTORY = 32 # Number of history card slots in state (32 * 13 bits)\n",
    "NUM_CARDS_TABLE = 3  # Number of table card slots in state (3 * 13 bits)\n",
    "NUM_CARDS_HAND = 9   # Number of hand card slots in state (9 * 13 bits)\n",
    "\n",
    "# Calculate the starting index of the player's hand bits within the state_bits vector\n",
    "# Order from parser: History, Table, Hand, Shown, Trump\n",
    "HAND_START_BIT_INDEX = (NUM_CARDS_HISTORY * CARD_BITS) + (NUM_CARDS_TABLE * CARD_BITS)\n",
    "# 32 * 13 = 416\n",
    "# 3 * 13  = 39\n",
    "# HAND_START_BIT_INDEX = 416 + 39 = 455. The hand bits are from index 455 up to 455 + (9*13) = 455 + 117 = 572.\n",
    "\n",
    "\n",
    "# --- Default Training Hyperparameters ---\n",
    "# Can be overridden when instantiating the trainer class\n",
    "DEFAULT_TRAINING_PARAMS = {\n",
    "    'validation_split_size': 0.15,\n",
    "    'random_seed': 42,\n",
    "    'bc_batch_size': 32,\n",
    "    'bc_learning_rate': 3e-4,\n",
    "    'bc_n_epochs': 10,\n",
    "    'device': th.device(\"cuda\"),\n",
    "    'dataloader_num_workers': 0, # Start with 0 workers for simplicity, increase for speed\n",
    "    'dataloader_pin_memory': True,\n",
    "}\n",
    "\n",
    "\n",
    "# --- 1. Data Location Collection Helper Function ---\n",
    "# This function remains outside the class as it's a data loading utility\n",
    "def collect_bc_sample_locations_from_hdf5(filepath: str) -> Tuple[Optional[List[Tuple[str, int]]], int]:\n",
    "    \"\"\"\n",
    "    Collects locations (group_name, sample_index) for valid BC samples\n",
    "    from all groups in the HDF5 file. Does NOT load actual data into memory.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Optional[list], int]: A list of (group_name, index_in_group) tuples\n",
    "                                     and the total count of samples, or None and 0 on failure.\n",
    "    \"\"\"\n",
    "    sample_locations = []\n",
    "    total_samples_count = 0\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        logging.error(f\"HDF5 file not found at: {filepath}\")\n",
    "        return None, 0\n",
    "\n",
    "    try:\n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            # Get all group names\n",
    "            # game_groups = list(f.keys()) # Process all groups\n",
    "            #game_groups = list(f.keys()) # Process all groups initially\n",
    "            game_groups = list(f.keys())[:2000] # Example: Process only first 2000 games\n",
    "\n",
    "            logging.info(f\"Found {len(f.keys())} game groups total. Processing {len(game_groups)} groups for locations.\")\n",
    "\n",
    "            if not game_groups:\n",
    "                logging.error(\"No game groups found in the HDF5 file.\")\n",
    "                return None, 0\n",
    "\n",
    "            for i, group_name in enumerate(game_groups):\n",
    "                if (i + 1) % 200 == 0:\n",
    "                    logging.info(f\"Collecting locations from group {i+1}/{len(game_groups)}: {group_name}\")\n",
    "                try:\n",
    "                    group = f[group_name]\n",
    "                    # --- Check required keys (without reading data) ---\n",
    "                    if OBS_KEY not in group:\n",
    "                        logging.warning(f\"Skipping group '{group_name}': Missing dataset '{OBS_KEY}'\")\n",
    "                        continue\n",
    "                    if ACTION_BITS_KEY not in group:\n",
    "                        logging.error(f\"CRITICAL: Skipping group '{group_name}': Missing dataset '{ACTION_BITS_KEY}'.\")\n",
    "                        continue\n",
    "\n",
    "                    # Get dataset sizes and shapes without loading data\n",
    "                    obs_dset_shape = group[OBS_KEY].shape\n",
    "                    actions_bits_dset_shape = group[ACTION_BITS_KEY].shape\n",
    "\n",
    "                    # --- Basic Validation (shape only) ---\n",
    "                    if obs_dset_shape[0] != actions_bits_dset_shape[0]:\n",
    "                         logging.warning(f\"Skipping group '{group_name}': Data length mismatch ({obs_dset_shape[0]} vs {actions_bits_dset_shape[0]}).\")\n",
    "                         continue\n",
    "                    if obs_dset_shape[0] == 0:\n",
    "                         logging.info(f\"Skipping empty group '{group_name}'.\")\n",
    "                         continue\n",
    "                    if obs_dset_shape[1] != INPUT_DIM:\n",
    "                         logging.warning(f\"Skipping group '{group_name}': Observation dimension mismatch (Expected {INPUT_DIM}, Got {obs_dset_shape[1]}).\")\n",
    "                         continue\n",
    "                    if actions_bits_dset_shape[1] != CARD_BITS:\n",
    "                         logging.warning(f\"Skipping group '{group_name}': Action bits dimension mismatch (Expected {CARD_BITS}, Got {actions_bits_dset_shape[1]}).\")\n",
    "                         continue\n",
    "\n",
    "                    # Collect locations for all samples in this group that passed initial checks.\n",
    "                    # The action index derivation (card in hand check) will happen in the Dataset.\n",
    "                    for j in range(obs_dset_shape[0]):\n",
    "                        sample_locations.append((group_name, j))\n",
    "\n",
    "                    total_samples_count += obs_dset_shape[0] # Count all samples from valid groups\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing group '{group_name}' for locations: {e}\")\n",
    "                    traceback.print_exc()\n",
    "                    continue # Skip this group on error\n",
    "\n",
    "            logging.info(f\"Finished collecting locations for {len(game_groups)} groups. Total potential samples found: {len(sample_locations)}\")\n",
    "\n",
    "            return sample_locations, total_samples_count\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Failed to access HDF5 file '{filepath}' or process groups for locations: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None, 0\n",
    "\n",
    "\n",
    "# --- 2. Custom HDF5 Dataset Class ---\n",
    "# This class also remains outside the trainer class as it's a generic data interface\n",
    "class HDF5Dataset(Dataset):\n",
    "    \"\"\"Custom Dataset for reading observations and converting action bits\n",
    "       to indices from HDF5 file on the fly.\"\"\"\n",
    "\n",
    "    def __init__(self, hdf5_filepath: str, sample_locations: List[Tuple[str, int]]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hdf5_filepath (str): Path to the HDF5 file.\n",
    "            sample_locations (list): List of (group_name, index_in_group) tuples\n",
    "                                     representing the samples in this dataset split.\n",
    "        \"\"\"\n",
    "        self.hdf5_filepath = hdf5_filepath\n",
    "        self.sample_locations = sample_locations\n",
    "        self._file = None # HDF5 file handle, opened on first access\n",
    "\n",
    "        # Basic validation of locations structure (optional but good)\n",
    "        if not all(isinstance(loc, tuple) and len(loc) == 2 for loc in sample_locations):\n",
    "             logging.warning(\"Dataset: Sample locations list has unexpected format.\")\n",
    "        if not sample_locations:\n",
    "             logging.warning(\"Dataset: Initialized with an empty list of sample locations.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in this dataset split.\"\"\"\n",
    "        return len(self.sample_locations)\n",
    "\n",
    "    def _get_file(self):\n",
    "        \"\"\"Helper to open the HDF5 file. Designed to be safer with multiprocessing\n",
    "           by potentially opening a handle per worker process used by DataLoader.\"\"\"\n",
    "        # Check if the file is already open in this process/thread\n",
    "        # h5py File objects are not thread-safe. Opening per process is safer.\n",
    "        # A simple check for self._file being None works well with DataLoader's\n",
    "        # worker initialization if the dataset object is copied to workers.\n",
    "        if self._file is None:\n",
    "             try:\n",
    "                 # Using swmr=True (Single Writer Multiple Reader) might be necessary\n",
    "                 # if the HDF5 file could potentially be written to while reading,\n",
    "                 # but for a static file, 'r' mode is fine. swmr requires HDF5 1.9+\n",
    "                 # self._file = h5py.File(self.hdf5_filepath, 'r', swmr=True)\n",
    "                 self._file = h5py.File(self.hdf5_filepath, 'r')\n",
    "                 # logging.debug(f\"Opened HDF5 file {self.hdf5_filepath} in process {os.getpid()}\") # Optional: log file opening\n",
    "             except Exception as e:\n",
    "                  logging.critical(f\"Dataset Error: Failed to open HDF5 file {self.hdf5_filepath} in _get_file for process {os.getpid()}: {e}\")\n",
    "                  # In a real scenario, failing to open the file is critical.\n",
    "                  # Raising an error will stop the DataLoader worker or main process.\n",
    "                  raise\n",
    "        return self._file\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Ensures the HDF5 file is closed when the dataset object is garbage collected.\"\"\"\n",
    "        if self._file is not None:\n",
    "             try:\n",
    "                 self._file.close()\n",
    "                 # logging.debug(f\"Closed HDF5 file {self.hdf5_filepath} in __del__ for process {os.getpid()}\") # Optional: log file closing\n",
    "             except Exception as e:\n",
    "                 logging.error(f\"Dataset Error: Error closing HDF5 file {self.hdf5_filepath} in __del__: {e}\")\n",
    "             self._file = None # Clear the reference\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Reads a single sample from the HDF5 file and returns observation and action index.\n",
    "        Performs the action bit to index conversion.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve (from 0 to len - 1).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[th.Tensor, th.Tensor]: The observation (float32) and\n",
    "                                         the action index (int64).\n",
    "\n",
    "        Raises:\n",
    "            IndexError: If idx is out of the valid range.\n",
    "            RuntimeError: If data inconsistencies prevent deriving a valid action index.\n",
    "            Exception: For other errors during file reading.\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= len(self.sample_locations):\n",
    "            raise IndexError(f\"Dataset index ({idx}) out of range (0-{len(self.sample_locations)-1})\")\n",
    "\n",
    "        group_name, sample_index_in_group = self.sample_locations[idx]\n",
    "\n",
    "        # Use the helper to get the file object (opens if not already open in this process)\n",
    "        f = self._get_file()\n",
    "\n",
    "        try:\n",
    "            # Access the group and datasets\n",
    "            group = f[group_name]\n",
    "            # Using [index:index+1] slicing reads a single row and keeps the dimension (shape (1, D))\n",
    "            obs_data_single = group[OBS_KEY][sample_index_in_group:sample_index_in_group+1] # Shape (1, INPUT_DIM) bool\n",
    "            actions_bits_data_single = group[ACTION_BITS_KEY][sample_index_in_group:sample_index_in_group+1] # Shape (1, CARD_BITS) bool\n",
    "\n",
    "            # --- Convert Action Card Bits (1, 13) bool to Action Index (scalar) int64 ---\n",
    "            # Convert boolean arrays to integers for easier comparison/processing\n",
    "            state_vec_int_single = obs_data_single.astype(np.int8).squeeze(axis=0) # Shape (INPUT_DIM,) int8\n",
    "            played_card_bits_int_single = actions_bits_data_single.astype(np.int8).squeeze(axis=0) # Shape (CARD_BITS,) int8\n",
    "\n",
    "            # Initialize action_index BEFORE the loop and BEFORE the check below\n",
    "            action_index = -1\n",
    "\n",
    "            # Extract the player's hand bits from the single state vector\n",
    "            hand_bits_int_single = state_vec_int_single[HAND_START_BIT_INDEX : HAND_START_BIT_INDEX + NUM_CARDS_HAND * CARD_BITS]\n",
    "\n",
    "            # Find the index of the played_card_bits within the hand_bits\n",
    "            for card_idx_in_hand in range(NUM_CARDS_HAND):\n",
    "                start_idx = card_idx_in_hand * CARD_BITS\n",
    "                end_idx = start_idx + CARD_BITS\n",
    "                current_hand_card_bits_int = hand_bits_int_single[start_idx : end_idx]\n",
    "\n",
    "                # Compare the played card bits with the current hand card bits\n",
    "                if np.array_equal(played_card_bits_int_single, current_hand_card_bits_int):\n",
    "                    action_index = card_idx_in_hand # Found the index!\n",
    "                    break # Found the card in hand, its index is our action\n",
    "\n",
    "            # --- Error Handling for Data Inconsistency ---\n",
    "            # This check happens AFTER the loop finishes and action_index has been determined (or remained -1).\n",
    "            if action_index == -1:\n",
    "                 played_bits_list = played_card_bits_int_single.tolist()\n",
    "                 hand_bits_list = hand_bits_int_single.reshape(-1, CARD_BITS).tolist()\n",
    "                 error_msg = (f\"Dataset Error: Sample {idx} ({group_name}, {sample_index_in_group}): \"\n",
    "                              f\"Played card bits {played_bits_list} not found in hand bits {hand_bits_list}. \"\n",
    "                              \"This sample cannot be used for training.\")\n",
    "                 # Log the error for debugging data issues\n",
    "                 logging.error(error_msg)\n",
    "                 # Raise a RuntimeError to stop training for this data problem\n",
    "                 raise RuntimeError(error_msg)\n",
    "\n",
    "\n",
    "            # Convert numpy arrays to torch tensors - THIS IS *AFTER* the error check\n",
    "            # Squeeze removes the leading dimension of 1 from the [index:index+1] slice\n",
    "            obs_tensor = th.from_numpy(obs_data_single.astype(np.float32)).squeeze(axis=0) # Shape (INPUT_DIM,) float32\n",
    "            action_index_tensor = th.tensor(action_index, dtype=th.int64) # Scalar tensor (int64)\n",
    "\n",
    "            # Return a dictionary expected by the DataLoader/imitation trainer\n",
    "            return {\"obs\": obs_tensor, \"acts\": action_index_tensor}\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch any exception occurring during reading/processing this specific sample\n",
    "            logging.error(f\"Dataset Error: Error reading or processing sample {idx} ({group_name}, {sample_index_in_group}) in __getitem__: {e}\")\n",
    "            traceback.print_exc()\n",
    "            # Re-raise the exception to signal the error to the DataLoader/trainer\n",
    "            raise\n",
    "\n",
    "\n",
    "# --- 3. Trainer Class ---\n",
    "# This class encapsulates the training setup and execution\n",
    "class BCCardPolicyTrainer:\n",
    "    \"\"\"\n",
    "    Encapsulates the Behavioral Cloning training process for the Jass card policy.\n",
    "    Handles data loading setup, trainer initialization, training, and saving.\n",
    "    \"\"\"\n",
    "    def __init__(self, hdf5_filepath: str, save_path: str, params: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initializes the trainer.\n",
    "\n",
    "        Args:\n",
    "            hdf5_filepath (str): Path to the HDF5 file containing training data.\n",
    "            save_path (str): Base path for saving the trained policy.\n",
    "            params (Dict[str, Any]): Dictionary of training parameters.\n",
    "        \"\"\"\n",
    "        self.hdf5_filepath = hdf5_filepath\n",
    "        self.save_path = save_path\n",
    "        self.params = params # Store parameters\n",
    "\n",
    "        # Apply default parameters if not provided\n",
    "        for key, default_value in DEFAULT_TRAINING_PARAMS.items():\n",
    "            if key not in self.params:\n",
    "                self.params[key] = default_value\n",
    "\n",
    "        # Extract parameters for clarity\n",
    "        self.validation_split_size = self.params['validation_split_size']\n",
    "        self.random_seed = self.params['random_seed']\n",
    "        self.bc_batch_size = self.params['bc_batch_size']\n",
    "        self.bc_learning_rate = self.params['bc_learning_rate']\n",
    "        self.bc_n_epochs = self.params['bc_n_epochs']\n",
    "        self.device = self.params['device']\n",
    "        self.dataloader_num_workers = self.params['dataloader_num_workers']\n",
    "        self.dataloader_pin_memory = self.params['dataloader_pin_memory']\n",
    "\n",
    "        logging.info(f\"Trainer initialized with parameters: {self.params}\")\n",
    "        logging.info(f\"Using device for training: {self.device}\")\n",
    "\n",
    "\n",
    "        self.train_dataloader = None\n",
    "        # self.val_dataloader = None # Optional: for validation during training\n",
    "        self.bc_trainer = None\n",
    "        self.n_batches = 0 # Total training batches calculated later\n",
    "\n",
    "\n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"\n",
    "        Collects sample locations, splits them, and creates HDF5Dataset and DataLoaders.\n",
    "        \"\"\"\n",
    "        logging.info(\"Loading and preparing data...\")\n",
    "\n",
    "        # --- Collect Data Sample Locations ---\n",
    "        sample_locations, total_samples = collect_bc_sample_locations_from_hdf5(self.hdf5_filepath)\n",
    "\n",
    "        # --- Error Handling for Location Collection ---\n",
    "        if sample_locations is None:\n",
    "            logging.critical(\"Trainer Error: Failed to collect sample locations (function returned None). Check HDF5_FILE_PATH and file accessibility. Exiting.\")\n",
    "            return False # Indicate failure\n",
    "\n",
    "        if not sample_locations:\n",
    "             logging.critical(\"Trainer Error: No valid data sample locations were collected (list is empty). Ensure HDF5 file contains groups with required datasets and correct dimensions. Exiting.\")\n",
    "             return False # Indicate failure\n",
    "\n",
    "        logging.info(f\"Total valid data sample locations collected: {len(sample_locations)}.\")\n",
    "\n",
    "        # --- Split Sample Locations ---\n",
    "        logging.info(f\"Splitting sample locations (keeping {1-self.validation_split_size:.0%} for training)...\")\n",
    "        try:\n",
    "            train_locations, val_locations = train_test_split(\n",
    "                sample_locations,\n",
    "                test_size=self.validation_split_size,\n",
    "                random_state=self.random_seed,\n",
    "                shuffle=True,\n",
    "            )\n",
    "        except ValueError as e:\n",
    "             logging.critical(f\"Trainer Error: Failed to split data locations. This can happen if test_size is too large for the number of samples. Error: {e}. Exiting.\")\n",
    "             return False # Indicate failure\n",
    "\n",
    "        logging.info(f\"Training locations: {len(train_locations)}, Validation locations: {len(val_locations)}\")\n",
    "\n",
    "        # Free up memory from the original full locations list\n",
    "        del sample_locations\n",
    "\n",
    "        # --- Create HDF5 Dataset for Training ---\n",
    "        if not train_locations:\n",
    "            logging.critical(\"Trainer Error: No training samples available after splitting locations. Adjust split size or provide more data. Exiting.\")\n",
    "            return False # Indicate failure\n",
    "        train_dataset = HDF5Dataset(self.hdf5_filepath, train_locations)\n",
    "\n",
    "        # --- Create DataLoader from the Training Dataset ---\n",
    "        # This DataLoader will be used by the BC trainer\n",
    "        self.train_dataloader = data_utils.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.bc_batch_size,\n",
    "            shuffle=True, # Shuffle data for better training convergence\n",
    "            num_workers=self.dataloader_num_workers,\n",
    "            pin_memory=self.dataloader_pin_memory,\n",
    "        )\n",
    "        logging.info(f\"Created training DataLoader with batch_size={self.bc_batch_size}, num_workers={self.dataloader_num_workers}\")\n",
    "\n",
    "\n",
    "        # --- Optional: Create DataLoader for Validation ---\n",
    "        # You would use this if you configure the BC trainer for validation during training\n",
    "        # if val_locations:\n",
    "        #     self.val_dataloader = data_utils.DataLoader(\n",
    "        #         HDF5Dataset(self.hdf5_filepath, val_locations),\n",
    "        #         batch_size=self.bc_batch_size, # Validation batch size can be different\n",
    "        #         shuffle=False, # No need to shuffle validation data\n",
    "        #         num_workers=self.dataloader_num_workers,\n",
    "        #         pin_memory=self.dataloader_pin_memory,\n",
    "        #     )\n",
    "        #     logging.info(f\"Created validation DataLoader with batch_size={self.bc_batch_size}, num_workers={self.dataloader_num_workers}\")\n",
    "        # else:\n",
    "        #     logging.warning(\"No validation samples available after splitting locations. Skipping validation DataLoader creation.\")\n",
    "        #     self.val_dataloader = None\n",
    "\n",
    "\n",
    "        # Calculate total number of training batches needed\n",
    "        num_train_samples = len(train_dataset)\n",
    "        n_batches_per_epoch = num_train_samples // self.bc_batch_size\n",
    "        if num_train_samples % self.bc_batch_size != 0:\n",
    "            n_batches_per_epoch += 1\n",
    "        self.n_batches = n_batches_per_epoch * self.bc_n_epochs\n",
    "        # Ensure n_batches is at least 1 if there are samples\n",
    "        self.n_batches = max(1, self.n_batches) if num_train_samples > 0 else 0\n",
    "\n",
    "        logging.info(f\"Data preparation complete. Total training batches calculated: {self.n_batches}\")\n",
    "        return True # Indicate success\n",
    "\n",
    "\n",
    "    def setup_trainer(self):\n",
    "        \"\"\"Initializes the imitation.algorithms.bc.BC trainer.\"\"\"\n",
    "        if self.train_dataloader is None:\n",
    "            logging.critical(\"Trainer Error: Data not loaded. Call load_and_prepare_data() first. Exiting.\")\n",
    "            return False # Indicate failure\n",
    "\n",
    "        logging.info(\"Setting up BC trainer...\")\n",
    "\n",
    "        # --- Prepare Spaces for Imitation Library ---\n",
    "        # These match the expected output of HDF5Dataset.__getitem__ and DataLoader collation\n",
    "        observation_space = gym.spaces.Box(low=0, high=1, shape=(INPUT_DIM,), dtype=np.float32)\n",
    "        action_space = gym.spaces.Discrete(CARD_ACTION_DIM) # Discrete(9) for action index\n",
    "\n",
    "        # --- Initialize Behavioral Cloning Model ---\n",
    "        rng = np.random.default_rng(self.random_seed)\n",
    "\n",
    "        try:\n",
    "            self.bc_trainer = bc.BC(\n",
    "                observation_space=observation_space,\n",
    "                action_space=action_space,\n",
    "                demonstrations=self.train_dataloader, # Pass the DataLoader instance\n",
    "                batch_size=self.bc_batch_size, # Passed for internal logic/logging? DataLoader controls actual batch size\n",
    "                optimizer_kwargs=dict(lr=self.bc_learning_rate),\n",
    "                device=self.device, # Trainer and policy will be moved to this device\n",
    "                rng=rng,\n",
    "                # Note: loss_calculator is handled internally by bc.BC using Cross-Entropy Loss for Discrete actions.\n",
    "                # eval_dataloader=self.val_dataloader, # Pass validation loader if created\n",
    "                # eval_metrics=..., # Define appropriate metrics if evaluating\n",
    "            )\n",
    "            logging.info(\"BC trainer initialized successfully.\")\n",
    "            return True # Indicate success\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Trainer Error: Failed to initialize BC trainer: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return False # Indicate failure\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Runs the Behavioral Cloning training process.\"\"\"\n",
    "        if self.bc_trainer is None:\n",
    "            logging.critical(\"Trainer Error: Trainer not set up. Call setup_trainer() first. Exiting.\")\n",
    "            return False # Indicate failure\n",
    "\n",
    "        if self.n_batches == 0:\n",
    "            logging.warning(\"Trainer: No training batches calculated. Skipping training.\")\n",
    "            return True # Consider success if no training needed due to lack of data\n",
    "\n",
    "        logging.info(f\"Starting BC training for {self.bc_n_epochs} epochs totaling {self.n_batches} batches...\")\n",
    "\n",
    "        try:\n",
    "            # bc.BC.train takes total batches to run for.\n",
    "            # It iterates through the provided DataLoader to get batches.\n",
    "            log_interval = max(1, self.n_batches // 50) # Log ~50 times during training\n",
    "            logging.info(f\"Logging training progress every {log_interval} batches.\")\n",
    "            self.bc_trainer.train(n_batches=self.n_batches, log_interval=log_interval)\n",
    "\n",
    "            logging.info(\"BC training finished.\")\n",
    "            return True # Indicate success\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Trainer Error: An error occurred during training: {e}\")\n",
    "            traceback.print_exc()\n",
    "            # The device mismatch error would likely happen here\n",
    "            return False # Indicate failure\n",
    "\n",
    "\n",
    "    def save_policy(self):\n",
    "        \"\"\"Saves the trained policy model.\"\"\"\n",
    "        if self.bc_trainer is None or self.bc_trainer.policy is None:\n",
    "            logging.warning(\"Trainer: No policy to save. Training may have failed or was skipped.\")\n",
    "            return False # Indicate failure\n",
    "\n",
    "        if self.n_batches == 0: # Only attempt saving if training was intended\n",
    "             logging.warning(\"Trainer: Skipping policy save because no training batches were available.\")\n",
    "             return True # Consider success if saving was skipped intentionally\n",
    "\n",
    "        logging.info(f\"Attempting to save policy to {self.save_path}.zip\")\n",
    "        try:\n",
    "            # The policy object is stored in bc_trainer.policy.\n",
    "            # Save it using the underlying SB3 policy save method (.zip file).\n",
    "            save_path_full = f\"{self.save_path}.zip\"\n",
    "            self.bc_trainer.policy.save(save_path_full)\n",
    "            logging.info(f\"Policy saved successfully to {save_path_full}\")\n",
    "            return True # Indicate success\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Trainer Error: Error saving BC policy: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return False # Indicate failure\n",
    "\n",
    "\n",
    "    # Optional: Add a load_policy method here if needed for inference later\n",
    "    # def load_policy(self, model_path: str):\n",
    "    #     \"\"\"Loads a trained policy model.\"\"\"\n",
    "    #     try:\n",
    "    #         # Assuming it's an SB3 policy saved with .save()\n",
    "    #         self.policy = sb3.common.policies.deserialize_policy(model_path, device=self.device)\n",
    "    #         logging.info(f\"Policy loaded successfully from {model_path}\")\n",
    "    #         # You might need observation_space and action_space defined if loading policy standalone\n",
    "    #     except Exception as e:\n",
    "    #         logging.error(f\"Error loading policy from {model_path}: {e}\")\n",
    "    #         self.policy = None\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"--- Script Start ---\")\n",
    "\n",
    "    # --- Configure and Instantiate Trainer ---\n",
    "    # Use default parameters, or override specific ones here\n",
    "    trainer_params = DEFAULT_TRAINING_PARAMS.copy()\n",
    "    # Example override:\n",
    "    # trainer_params['bc_n_epochs'] = 20\n",
    "    # trainer_params['dataloader_num_workers'] = 4 # Increase workers for speed\n",
    "    # trainer_params['device'] = th.device(\"cuda:1\") # Specify a different GPU if available\n",
    "\n",
    "    logging.info(f\"Using device: {trainer_params['device']}\")\n",
    "\n",
    "\n",
    "    trainer = BCCardPolicyTrainer(\n",
    "        hdf5_filepath=HDF5_FILE_PATH,\n",
    "        save_path=POLICY_SAVE_PATH,\n",
    "        params=trainer_params # Pass configuration parameters\n",
    "    )\n",
    "\n",
    "    # --- Execute Training Workflow ---\n",
    "    # 1. Load and prepare data\n",
    "    if not trainer.load_and_prepare_data():\n",
    "        logging.critical(\"Data preparation failed. Exiting.\")\n",
    "        exit() # Exit if data loading/preparation failed\n",
    "\n",
    "    # 2. Setup the BC trainer model\n",
    "    if not trainer.setup_trainer():\n",
    "         logging.critical(\"Trainer setup failed. Exiting.\")\n",
    "         exit() # Exit if trainer initialization failed\n",
    "\n",
    "    # 3. Run the training process\n",
    "    # The device mismatch error would likely happen inside trainer.train()\n",
    "    if not trainer.train():\n",
    "         logging.critical(\"Training process failed. Exiting.\")\n",
    "         exit() # Exit if training encountered a critical error\n",
    "\n",
    "    # 4. Save the trained policy\n",
    "    if not trainer.save_policy():\n",
    "         logging.error(\"Policy saving failed.\")\n",
    "         # Do not necessarily exit, training might have succeeded but saving failed\n",
    "\n",
    "\n",
    "    logging.info(\"--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847d3eea-a123-4af3-a1ac-ff6b32e0aadb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
