# -*- coding: utf-8 -*-
"""
train_card_policy_bc.py

Trains a card prediction policy using Behavioral Cloning (BC)
based on data parsed into an HDF5 file.

Assumes the HDF5 file contains 'state_bits' (bool, N x 929) and
'action_card_bits' (bool, N x 13) datasets, as generated by the parser.
Converts action_card_bits to action_index (int, N) during loading using a
custom HDF5 dataset to handle large files without loading everything into memory.

Uses the standard imitation.algorithms.bc.BC trainer.
Requires compatible versions of torch, gymnasium, stable-baselines3, imitation, and h5py.
"""

import h5py
import numpy as np
import gymnasium as gym
# stable_baselines3 is implicitly used by imitation for policy structure
import stable_baselines3 as sb3
# Import core BC algorithm and data types
from imitation.algorithms import bc
from imitation.data import types
# Need Dataset for custom data loading
from torch.utils.data import Dataset

import logging
import os
import torch as th # Use th alias for torch conventions
import traceback # To print full error tracebacks
from typing import Tuple, Optional, List
from sklearn.model_selection import train_test_split


# --- Logging Configuration ---
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')


# --- Configuration ---
HDF5_FILE_PATH = r'../../../../Training_Data/jass.hdf5' # <--- Path to the HDF5 file from your parser
POLICY_SAVE_PATH = 'jass_bc_card_policy_591_hdf5dataset' # Model save path

# --- Data Keys (Must match your parser's output HDF5 structure) ---
OBS_KEY = 'state_bits'         # N x 929 boolean array
ACTION_BITS_KEY = 'action_card_bits' # N x 13 boolean array (the played card's representation)

# --- Model Dimensions (Based on your parser) ---
INPUT_DIM = 929 # From your parser's STATE_BITS calculation
CARD_ACTION_DIM = 9 # 9 possible card positions in hand (index 0-8)

# --- Constants from Parser (Needed to interpret state_bits) ---
CARD_BITS = 13 # Bits used to represent a single card in the parser
NUM_CARDS_HISTORY = 32 # Number of history card slots in state (32 * 13 bits)
NUM_CARDS_TABLE = 3  # Number of table card slots in state (3 * 13 bits) - matches parser code, not comment
NUM_CARDS_HAND = 9   # Number of hand card slots in state (9 * 13 bits) - matches CARD_ACTION_DIM conceptually

# Calculate the starting index of the player's hand bits within the state_bits vector
# Order from parser: History, Table, Hand, Shown, Trump
HAND_START_BIT_INDEX = (NUM_CARDS_HISTORY * CARD_BITS) + (NUM_CARDS_TABLE * CARD_BITS)
# 32 * 13 = 416
# 3 * 13  = 39
# HAND_START_BIT_INDEX = 416 + 39 = 455. The hand bits are from index 455 up to 455 + (9*13) = 455 + 117 = 572.


# --- Training Hyperparameters ---
VALIDATION_SPLIT_SIZE = 0.15 # Use 15% of data for validation (optional for BC eval)
RANDOM_SEED = 42 # For reproducible train/test splits
BC_BATCH_SIZE = 32
BC_LEARNING_RATE = 3e-4
BC_N_EPOCHS = 10 # Number of passes over the training data

# Use GPU if available, otherwise CPU
device = th.device("cpu")
logging.info(f"Using device: {device}")


# --- 1. Modified Load Function (Collects Sample Locations Only) ---
def collect_bc_sample_locations_from_hdf5(filepath: str) -> Tuple[Optional[List[Tuple[str, int]]], int]:
    """
    Collects locations (group_name, sample_index) for valid BC samples
    from all groups in the HDF5 file. Does NOT load actual data into memory.

    Returns:
        Tuple[Optional[list], int]: A list of (group_name, index_in_group) tuples
                                     and the total count of samples, or None and 0 on failure.
    """
    sample_locations = []
    total_samples_count = 0

    if not os.path.exists(filepath):
        logging.error(f"HDF5 file not found at: {filepath}")
        return None, 0

    try:
        # Open file in read mode
        with h5py.File(filepath, 'r') as f:
            # Get all group names
            # game_groups = list(f.keys()) # Process all groups
            game_groups = list(f.keys())
            logging.info(f"Found {len(f.keys())} game groups total. Processing {len(game_groups)} groups for locations.")


            if not game_groups:
                logging.error("No game groups found in the HDF5 file.")
                return None, 0

            for i, group_name in enumerate(game_groups):
                if (i + 1) % 200 == 0:
                    logging.info(f"Collecting locations from group {i+1}/{len(game_groups)}: {group_name}")
                try:
                    group = f[group_name]
                    # --- Check required keys (without reading data) ---
                    if OBS_KEY not in group:
                        logging.warning(f"Skipping group '{group_name}': Missing dataset '{OBS_KEY}'")
                        continue
                    if ACTION_BITS_KEY not in group:
                        logging.error(f"CRITICAL: Skipping group '{group_name}': Missing dataset '{ACTION_BITS_KEY}'.")
                        continue

                    # Get dataset sizes and shapes without loading data
                    obs_dset_shape = group[OBS_KEY].shape
                    actions_bits_dset_shape = group[ACTION_BITS_KEY].shape

                    # --- Basic Validation (shape only) ---
                    if obs_dset_shape[0] != actions_bits_dset_shape[0]:
                         logging.warning(f"Skipping group '{group_name}': Data length mismatch ({obs_dset_shape[0]} vs {actions_bits_dset_shape[0]}).")
                         continue
                    if obs_dset_shape[0] == 0:
                         logging.info(f"Skipping empty group '{group_name}'.")
                         continue
                    if obs_dset_shape[1] != INPUT_DIM:
                         logging.warning(f"Skipping group '{group_name}': Observation dimension mismatch (Expected {INPUT_DIM}, Got {obs_dset_shape[1]}).")
                         continue
                    if actions_bits_dset_shape[1] != CARD_BITS:
                         logging.warning(f"Skipping group '{group_name}': Action bits dimension mismatch (Expected {CARD_BITS}, Got {actions_bits_dset_shape[1]}).")
                         continue

                    # Collect locations for all samples in this group that passed initial checks.
                    # The action index derivation (card in hand check) will happen in the Dataset.
                    for j in range(obs_dset_shape[0]):
                        sample_locations.append((group_name, j))

                    total_samples_count += obs_dset_shape[0] # Count all samples from valid groups

                except Exception as e:
                    logging.error(f"Error processing group '{group_name}' for locations: {e}")
                    traceback.print_exc()
                    continue # Skip this group on error

            logging.info(f"Finished collecting locations for {len(game_groups)} groups. Total potential samples found: {len(sample_locations)}")

            return sample_locations, total_samples_count

    except Exception as e:
        logging.critical(f"Failed to access HDF5 file '{filepath}' or process groups for locations: {e}")
        traceback.print_exc()
        return None, 0

# --- 2. Custom HDF5 Dataset Class ---
class HDF5Dataset(Dataset):
    """Custom Dataset for reading observations and converting action bits
       to indices from HDF5 file on the fly."""

    def __init__(self, hdf5_filepath: str, sample_locations: List[Tuple[str, int]]):
        """
        Args:
            hdf5_filepath (str): Path to the HDF5 file.
            sample_locations (list): List of (group_name, index_in_group) tuples
                                     representing the samples in this dataset split.
        """
        self.hdf5_filepath = hdf5_filepath
        self.sample_locations = sample_locations
        self._file = None # HDF5 file handle, opened on first access

        # Basic validation of locations structure (optional but good)
        if not all(isinstance(loc, tuple) and len(loc) == 2 for loc in sample_locations):
             logging.warning("Sample locations list has unexpected format.")
        if not sample_locations:
             logging.warning("Dataset initialized with an empty list of sample locations.")


    def __len__(self):
        """Returns the total number of samples in this dataset split."""
        return len(self.sample_locations)

    def _get_file(self):
        """Helper to open the HDF5 file. Designed to be safer with multiprocessing
           by potentially opening a handle per worker process."""
        # Check if the file is already open in this process/thread
        # h5py File objects are not thread-safe. Opening per process is safer.
        # A simple check for self._file being None works well with DataLoader's
        # worker initialization if the dataset object is copied to workers.
        if self._file is None:
             try:
                 # Using swmr=True (Single Writer Multiple Reader) might be necessary
                 # if the HDF5 file could potentially be written to while reading,
                 # but for a static file, 'r' mode is fine. swmr requires HDF5 1.9+
                 # self._file = h5py.File(self.hdf5_filepath, 'r', swmr=True)
                 self._file = h5py.File(self.hdf5_filepath, 'r')
                 # logging.debug(f"Opened HDF5 file {self.hdf5_filepath} in process {os.getpid()}") # Optional: log file opening
             except Exception as e:
                  logging.critical(f"Failed to open HDF5 file {self.hdf5_filepath} in _get_file for process {os.getpid()}: {e}")
                  # In a real scenario, failing to open the file is critical.
                  # Raising an error will stop the DataLoader worker or main process.
                  raise
        return self._file

    def __del__(self):
        """Ensures the HDF5 file is closed when the dataset object is garbage collected."""
        if self._file is not None:
             try:
                 self._file.close()
                 # logging.debug(f"Closed HDF5 file {self.hdf5_filepath} in __del__ for process {os.getpid()}") # Optional: log file closing
             except Exception as e:
                 logging.error(f"Error closing HDF5 file {self.hdf5_filepath} in __del__: {e}")
             self._file = None # Clear the reference


    # ... (inside HDF5Dataset class) ...

    # ... (inside HDF5Dataset class) ...

    def __getitem__(self, idx):
        """
        Reads a single sample from the HDF5 file and returns observation and action index.
        Performs the action bit to index conversion.

        Args:
            idx (int): Index of the sample to retrieve (from 0 to len - 1).

        Returns:
            Tuple[th.Tensor, th.Tensor]: The observation (float32) and
                                         the action index (int64).

        Raises:
            IndexError: If idx is out of the valid range.
            RuntimeError: If data inconsistencies prevent deriving a valid action index.
            Exception: For other errors during file reading.
        """
        if idx < 0 or idx >= len(self.sample_locations):
            raise IndexError(f"Dataset index ({idx}) out of range (0-{len(self.sample_locations)-1})")

        group_name, sample_index_in_group = self.sample_locations[idx]

        # Use the helper to get the file object (opens if not already open in this process)
        f = self._get_file()

        try:
            # Access the group and datasets
            group = f[group_name]
            # Using [index:index+1] slicing reads a single row and keeps the dimension (shape (1, D))
            obs_data_single = group[OBS_KEY][sample_index_in_group:sample_index_in_group+1] # Shape (1, INPUT_DIM) bool
            actions_bits_data_single = group[ACTION_BITS_KEY][sample_index_in_group:sample_index_in_group+1] # Shape (1, CARD_BITS) bool

            # --- Convert Action Card Bits (1, 13) bool to Action Index (scalar) int64 ---
            # Convert boolean arrays to integers for easier comparison/processing
            state_vec_int_single = obs_data_single.astype(np.int8).squeeze(axis=0) # Shape (INPUT_DIM,) int8
            played_card_bits_int_single = actions_bits_data_single.astype(np.int8).squeeze(axis=0) # Shape (CARD_BITS,) int8

            # Initialize action_index BEFORE the loop and BEFORE the check below
            action_index = -1

            # Extract the player's hand bits from the single state vector
            hand_bits_int_single = state_vec_int_single[HAND_START_BIT_INDEX : HAND_START_BIT_INDEX + NUM_CARDS_HAND * CARD_BITS]

            # Find the index of the played_card_bits within the hand_bits
            for card_idx_in_hand in range(NUM_CARDS_HAND):
                start_idx = card_idx_in_hand * CARD_BITS
                end_idx = start_idx + CARD_BITS
                current_hand_card_bits_int = hand_bits_int_single[start_idx : end_idx]

                # Compare the played card bits with the current hand card bits
                if np.array_equal(played_card_bits_int_single, current_hand_card_bits_int):
                    action_index = card_idx_in_hand # Found the index!
                    break # Found the card in hand, its index is our action

            # --- Error Handling for Data Inconsistency ---
            # This check should happen AFTER the loop finishes and action_index has been determined (or remained -1).
            if action_index == -1:
                 played_bits_list = played_card_bits_int_single.tolist()
                 hand_bits_list = hand_bits_int_single.reshape(-1, CARD_BITS).tolist()
                 error_msg = (f"CRITICAL Data Inconsistency: Sample {idx} ({group_name}, {sample_index_in_group}): "
                              f"Played card bits {played_bits_list} not found in hand bits {hand_bits_list}. "
                              "This sample cannot be used for training.")
                 # Log the error for debugging data issues
                 logging.error(error_msg)
                 # Raise a RuntimeError to stop training for this data problem
                 raise RuntimeError(error_msg)


            # Convert numpy arrays to torch tensors - THIS IS *AFTER* the error check
            # Squeeze removes the leading dimension of 1 from the [index:index+1] slice
            obs_tensor = th.from_numpy(obs_data_single.astype(np.float32)).squeeze(axis=0) # Shape (INPUT_DIM,) float32
            action_index_tensor = th.tensor(action_index, dtype=th.int64) # Scalar tensor (int64)

            # Return a dictionary expected by the DataLoader/imitation trainer
            return {"obs": obs_tensor, "acts": action_index_tensor}

        except Exception as e:
            # Catch any exception occurring during reading/processing this specific sample
            logging.error(f"Error reading or processing sample {idx} ({group_name}, {sample_index_in_group}) in __getitem__: {e}")
            traceback.print_exc()
            # Re-raise the exception to signal the error to the DataLoader/trainer
            raise


# ... (rest of the script) ...

# --- Main Execution ---
if __name__ == "__main__":
    logging.info("--- Starting Card Policy Training (BC) ---")

    # --- Collect Data Sample Locations ---
    # We now collect locations, not load all data into memory
    sample_locations, total_samples = collect_bc_sample_locations_from_hdf5(HDF5_FILE_PATH)

    # --- Error Handling for Location Collection ---
    # Explicitly check if the function returned None.
    if sample_locations is None:
        logging.critical("Failed to collect sample locations (function returned None). Check HDF5_FILE_PATH and file accessibility. Exiting.")
        exit()

    # Check if the returned list of locations is empty.
    if not sample_locations: # This handles the case where the list is []
         logging.critical("No valid data sample locations were collected (list is empty after processing groups). Ensure HDF5 file contains groups with required datasets and correct dimensions. Exiting.")
         exit()

    # If we reach here, sample_locations is guaranteed to be a non-empty list of tuples.
    logging.info(f"Total valid data sample locations collected for BC: {len(sample_locations)}.")


    # --- Split Sample Locations ---
    logging.info(f"Splitting sample locations for BC (keeping {1-VALIDATION_SPLIT_SIZE:.0%} for training)...")
    # sample_locations is guaranteed to be a non-empty list here, so train_test_split will work.
    train_locations, val_locations = train_test_split(
        sample_locations,
        test_size=VALIDATION_SPLIT_SIZE,
        random_state=RANDOM_SEED,
        shuffle=True,
        # Stratify option requires pre-calculating labels for each location if needed.
        # For randomness, splitting locations directly is fine.
    )
    logging.info(f"BC Training locations: {len(train_locations)}, Validation locations: {len(val_locations)}")

    # Free up memory from the original full locations list as it's no longer needed
    del sample_locations

    # --- Create HDF5 Dataset for Training ---
    # Check if the training split resulted in any samples
    if not train_locations:
        logging.critical("No training samples available after splitting locations. Adjust split size or provide more data. Exiting.")
        exit()
    train_dataset = HDF5Dataset(HDF5_FILE_PATH, train_locations)

    # --- Create DataLoader from the Training Dataset ---
    # This is the key change: Pass the DataLoader instance to BC
    # The DataLoader will handle batching and reading from the HDF5Dataset
    train_dataloader = th.utils.data.DataLoader(
        train_dataset,
        batch_size=BC_BATCH_SIZE, # Use the desired batch size for training steps
        shuffle=True,              # Shuffle data for better training convergence
        # Add num_workers > 0 for parallel data loading (can speed things up, but
        # requires robust HDF5 file handling in the Dataset's __getitem__ method)
        num_workers=min(os.cpu_count(), 4), # Example: Use up to 4 workers
        # pin_memory=True if device.type == 'cuda' else False, # Speeds up GPU transfer
    )

    # You could optionally create a DataLoader for the validation dataset as well
    # if val_locations:
    #     val_dataset = HDF5Dataset(HDF5_FILE_PATH, val_locations)
    #     val_dataloader = th.utils.data.DataLoader(val_dataset, batch_size=BC_BATCH_SIZE)
    # else:
    #     logging.warning("No validation samples available after splitting locations.")
    #     val_dataloader = None # Explicitly set to None if empty


    # The BC trainer expects demonstrations in a specific format, and it seems passing
    # a DataLoader is the expected way for this version of imitation.
    train_demonstrations = train_dataloader # Pass the DataLoader instance


    # --- Prepare Spaces for Imitation Library ---
    # Spaces are defined based on the *expected* data format from the Dataset's __getitem__
    # Ensure these match the output of HDF5Dataset.__getitem__ (float32 obs, int64 action index)
    observation_space = gym.spaces.Box(low=0, high=1, shape=(INPUT_DIM,), dtype=np.float32)
    action_space = gym.spaces.Discrete(CARD_ACTION_DIM) # Keep Discrete(9) for action index


    # --- Train Behavioral Cloning Model ---
    rng = np.random.default_rng(RANDOM_SEED)
    bc_trainer = bc.BC(
        observation_space=observation_space,
        action_space=action_space,
        demonstrations=train_demonstrations, # NOW passing the DataLoader
        # Keep batch_size here - it might be used internally by BC even with a DataLoader
        # or for logging purposes, but the DataLoader controls the actual batch size.
        batch_size=BC_BATCH_SIZE,
        optimizer_kwargs=dict(lr=BC_LEARNING_RATE),
        device=device, # Trainer and policy will be moved to this device
        rng=rng,
        # Note: loss_calculator is handled internally by bc.BC using Cross-Entropy Loss
        # for Discrete action spaces, which is what we want for classification (0-8).
        # If using evaluation during training, you would add:
        # eval_dataloader=val_dataloader,
        # eval_metrics=..., # Define appropriate metrics if evaluating
    )

    # Calculate n_batches based on the total number of SAMPLES in the training dataset
    # The DataLoader (used internally by bc.BC.train) handles iterating over the dataset
    # and producing batches. We tell train() how many total batches (training steps) to run.
    num_train_samples = len(train_dataset) # Get size from the dataset
    n_batches_per_epoch = num_train_samples // BC_BATCH_SIZE
    if num_train_samples % BC_BATCH_SIZE != 0:
        n_batches_per_epoch += 1 # Account for the last partial batch

    n_batches = n_batches_per_epoch * BC_N_EPOCHS
    # Ensure n_batches is at least 1 if there's data to train on
    n_batches = max(1, n_batches) if num_train_samples > 0 else 0


    logging.info(f"Starting BC training for {BC_N_EPOCHS} epochs (~{n_batches_per_epoch} batches per epoch) totaling {n_batches} batches...")
    if n_batches > 0:
        # bc.BC.train takes total batches to run for. It iterates through the dataset
        # using the provided DataLoader to get batches.
        log_interval = max(1, n_batches // 50) # Log ~50 times during training
        logging.info(f"Logging training progress every {log_interval} batches.")
        bc_trainer.train(n_batches=n_batches, log_interval=log_interval)
    else:
        logging.warning("No training batches available. Skipping training.")


    # --- Save Policy ---
    if n_batches > 0: # Only attempt to save if training actually ran
        try:
            # The policy object is stored in bc_trainer.policy.
            # Save it using the underlying SB3 policy save method (.zip file).
            save_path = f"{POLICY_SAVE_PATH}.zip"
            bc_trainer.policy.save(save_path)
            logging.info(f"Card prediction policy saved successfully to {save_path}")
        except Exception as e:
            logging.error(f"Error saving BC policy: {e}")
            traceback.print_exc()
    else:
        logging.warning("Skipping policy save because no training was performed (n_batches was 0).")


    logging.info("--- Card Policy Training Finished ---")