{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "097f40ce-bf90-4923-a11b-f63cd2ab8a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 14:55:21 - INFO - Using device: cpu\n",
      "2025-04-20 14:55:21 - INFO - --- Starting Card Policy Training (BC) ---\n",
      "2025-04-20 14:55:21 - INFO - Scanning HDF5 file '../../../Training_Data/jass.hdf5' to identify valid samples...\n",
      "2025-04-20 14:55:28 - INFO - Scanning 2000 game groups in HDF5 file for valid samples...\n",
      "2025-04-20 14:55:30 - INFO - Scan Skipping empty group 'S2500_00ac20ed3234029404a30fb0e06d7109'.\n",
      "2025-04-20 14:55:33 - INFO - Scan Skipping empty group 'S2500_0262972aa114bc043c893e8a1e99aad1'.\n",
      "2025-04-20 14:55:36 - INFO - Scanning group 500/2000: S2500_03c94bff41ef615ddec3bdac0d88ea9a\n",
      "2025-04-20 14:55:38 - INFO - Scan Skipping empty group 'S2500_049292ad4c8103be55bcc178aa3a3620'.\n",
      "2025-04-20 14:55:42 - INFO - Scan Skipping empty group 'S2500_06a7d30b3751fde4fc347acb627b58c8'.\n",
      "2025-04-20 14:55:44 - INFO - Scanning group 1000/2000: S2500_0730d9d611061cc0a24fb83c62cdca11\n",
      "2025-04-20 14:55:50 - INFO - Scanning group 1500/2000: S2500_0b05817feed1dd5180a24f86a4df7b1a\n",
      "2025-04-20 14:55:55 - INFO - Scan Skipping empty group 'S2500_0d7d32a5ac1f42604f63ddb1e9d9dcea'.\n",
      "2025-04-20 14:55:55 - INFO - Scan Skipping empty group 'S2500_0daa80e046d7f2c7d12a604560a360ef'.\n",
      "2025-04-20 14:55:57 - INFO - Scanning group 2000/2000: S2500_0e83c87bdc2fce1894c8d078c1850b25\n",
      "2025-04-20 14:55:57 - INFO - Scan complete. Identified 734009 valid samples across all groups.\n",
      "2025-04-20 14:55:57 - INFO - Total valid data samples identified: 734009.\n",
      "2025-04-20 14:55:57 - INFO - Splitting valid samples (keeping 85% for training)...\n",
      "2025-04-20 14:55:57 - INFO - BC Training samples: 623907, Validation samples: 110102\n",
      "2025-04-20 14:55:57 - INFO - Initialized JassCardDataset with 623907 samples.\n",
      "2025-04-20 14:55:57 - INFO - Created DataLoader with batch_size=32, num_workers=0.\n",
      "2025-04-20 14:56:00 - INFO - Starting BC training for 10 epochs using DataLoader...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BC.train() got an unexpected keyword argument 'data_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 328\u001b[0m\n\u001b[1;32m    324\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting BC training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBC_N_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs using DataLoader...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_dataset) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;66;03m# When using data_loader, train method uses n_epochs argument directly.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# The loader yields batches, and train runs for the specified number of epochs.\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m     \u001b[43mbc_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBC_N_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# <-- ADD data_loader here\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;66;03m# Optional: Add validation evaluation here after training if val_samples_list is kept\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo training samples available. Skipping training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: BC.train() got an unexpected keyword argument 'data_loader'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "train_card_policy_bc.py\n",
    "\n",
    "Trains a card prediction policy using Behavioral Cloning (BC)\n",
    "based on data parsed into an HDF5 file using a manual training loop.\n",
    "\n",
    "Assumes the HDF5 file contains 'state_bits' (bool, N x 929) and\n",
    "'action_card_bits' (bool, N x 13) datasets, as generated by the parser.\n",
    "Converts action_card_bits to action_index (int, N) during loading.\n",
    "\n",
    "Implements a custom PyTorch Dataset and DataLoader to avoid loading the\n",
    "entire dataset into memory at once, addressing potential Out-of-Memory errors.\n",
    "The DataLoader reads data samples on-demand from the HDF5 file during training.\n",
    "\"\"\"\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "# stable_baselines3 is implicitly used by imitation for policy structure\n",
    "import stable_baselines3 as sb3\n",
    "# Import core BC algorithm and data types\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import types\n",
    "# Need PyTorch Dataset and DataLoader for memory-efficient loading\n",
    "import torch as th\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import traceback\n",
    "from typing import Tuple, Optional, List, Any\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Logging Configuration ---\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# --- Configuration ---\n",
    "HDF5_FILE_PATH = r'../../../Training_Data/jass.hdf5' # <--- Path to the HDF5 file from your parser\n",
    "POLICY_SAVE_PATH = 'jass_bc_card_policy_591' # Model save path\n",
    "\n",
    "# --- Data Keys (Must match your parser's output HDF5 structure) ---\n",
    "OBS_KEY = 'state_bits'         # N x 929 boolean array\n",
    "ACTION_BITS_KEY = 'action_card_bits' # N x 13 boolean array (the played card's representation)\n",
    "\n",
    "# --- Model Dimensions (Based on your parser) ---\n",
    "INPUT_DIM = 929 # From your parser's STATE_BITS calculation\n",
    "CARD_ACTION_DIM = 9 # 9 possible card positions in hand (index 0-8)\n",
    "\n",
    "# --- Constants from Parser (Needed to interpret state_bits) ---\n",
    "CARD_BITS = 13 # Bits used to represent a single card in the parser\n",
    "NUM_CARDS_HISTORY = 32 # Number of history card slots in state (32 * 13 bits)\n",
    "NUM_CARDS_TABLE = 3  # Number of table card slots in state (3 * 13 bits) - matches parser code, not comment\n",
    "NUM_CARDS_HAND = 9   # Number of hand card slots in state (9 * 13 bits) - matches CARD_ACTION_DIM conceptually\n",
    "\n",
    "# Calculate the starting index of the player's hand bits within the state_bits vector\n",
    "# Order from parser: History, Table, Hand, Shown, Trump\n",
    "HAND_START_BIT_INDEX = (NUM_CARDS_HISTORY * CARD_BITS) + (NUM_CARDS_TABLE * CARD_BITS)\n",
    "# 32 * 13 = 416\n",
    "# 3 * 13  = 39\n",
    "# HAND_START_BIT_INDEX = 416 + 39 = 455. The hand bits are from index 455 up to 455 + (9*13) = 455 + 117 = 572.\n",
    "\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "VALIDATION_SPLIT_SIZE = 0.15 # Use 15% of data for validation (optional)\n",
    "RANDOM_SEED = 42 # For reproducible train/test splits\n",
    "BC_BATCH_SIZE = 32\n",
    "BC_LEARNING_RATE = 3e-4\n",
    "BC_N_EPOCHS = 10 # Number of passes over the training data\n",
    "BC_NUM_WORKERS = 0 # Number of subprocesses for data loading (0 means main process). Start with 0 to debug HDF5 access issues with multiprocessing. Increase later if needed and HDF5 access is thread-safe or handled properly.\n",
    "\n",
    "# --- Set device to attempt CUDA ---\n",
    "device = th.device(\"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# --- 1. Custom Dataset for Memory-Efficient Loading ---\n",
    "\n",
    "class JassCardDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset to load observations and action indices\n",
    "    on-demand from an HDF5 file, avoiding loading everything into memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath: str, sample_list: List[Tuple[str, int, int]]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filepath (str): Path to the HDF5 file.\n",
    "            sample_list (List[Tuple[str, int, int]]): A list of tuples where each tuple\n",
    "                is (group_name, index_in_group, derived_action_index). This list\n",
    "                is generated by scanning the HDF5 file *initially* to identify\n",
    "                valid samples and their corresponding action indices.\n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.sample_list = sample_list\n",
    "        logging.info(f\"Initialized JassCardDataset with {len(self.sample_list)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.sample_list)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        Loads and returns the observation and action index for a given index `idx`.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index in the `self.sample_list`.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[th.Tensor, th.Tensor]: A tuple containing the observation tensor\n",
    "                (float32) and the action index tensor (int64).\n",
    "        \"\"\"\n",
    "        # Get the pre-calculated information for this sample\n",
    "        group_name, sample_idx_in_group, derived_action_index = self.sample_list[idx]\n",
    "\n",
    "        try:\n",
    "            # Open the HDF5 file in __getitem__ for robustness with num_workers > 0\n",
    "            # (Each worker process will open its own file handle)\n",
    "            with h5py.File(self.filepath, 'r') as f:\n",
    "                # Read only the observation for this specific sample\n",
    "                # Access dataset via group, then slice to get one row\n",
    "                obs_data = f[group_name][OBS_KEY][sample_idx_in_group, :] # Shape (INPUT_DIM,)\n",
    "\n",
    "            # Convert observation data to float32 PyTorch tensor\n",
    "            obs_tensor = th.tensor(obs_data, dtype=th.float32)\n",
    "\n",
    "            # Convert the pre-calculated action index to int64 PyTorch tensor\n",
    "            action_tensor = th.tensor(derived_action_index, dtype=th.int64)\n",
    "\n",
    "            return obs_tensor, action_tensor\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading data for index {idx} ({group_name}/{sample_idx_in_group}) from HDF5: {e}\")\n",
    "            # Depending on how critical corrupted samples are, you could return\n",
    "            # dummy data, raise an error, or have the DataLoader's collate_fn handle it.\n",
    "            # Returning zeros might corrupt training. For now, let's log and raise,\n",
    "            # or rely on DataLoader error handling. A better approach for training\n",
    "            # is usually to filter invalid samples *before* creating the sample_list.\n",
    "            # Since we filter in scan_hdf5_for_valid_samples, this exception\n",
    "            # indicates a read error, not a data validation error.\n",
    "            raise e # Re-raise the exception\n",
    "\n",
    "\n",
    "# --- 2. Function to Scan HDF5 and Identify Valid Samples ---\n",
    "\n",
    "def scan_hdf5_for_valid_samples(filepath: str) -> List[Tuple[str, int, int]]:\n",
    "    \"\"\"\n",
    "    Scans the HDF5 file group by group to identify valid state/action pairs\n",
    "    and derive the action index (0-8). Returns a list of tuples containing\n",
    "    (group_name, index_in_group, derived_action_index) for each valid sample.\n",
    "    This avoids loading all observations into memory at once, but loads\n",
    "    each group's data temporarily.\n",
    "    \"\"\"\n",
    "    valid_samples = []\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        logging.error(f\"HDF5 file not found at: {filepath}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            game_groups = list(f.keys())[:2000] # Load all groups\n",
    "            logging.info(f\"Scanning {len(game_groups)} game groups in HDF5 file for valid samples...\")\n",
    "\n",
    "            if not game_groups:\n",
    "                logging.error(\"No game groups found in the HDF5 file.\")\n",
    "                return []\n",
    "\n",
    "            for i, group_name in enumerate(game_groups):\n",
    "                if (i + 1) % 500 == 0: # Log progress every 500 groups\n",
    "                     logging.info(f\"Scanning group {i+1}/{len(game_groups)}: {group_name}\")\n",
    "                try:\n",
    "                    group = f[group_name]\n",
    "                    # --- Check required keys ---\n",
    "                    if OBS_KEY not in group:\n",
    "                        logging.warning(f\"Scan Skipping group '{group_name}': Missing dataset '{OBS_KEY}'\")\n",
    "                        continue\n",
    "                    if ACTION_BITS_KEY not in group:\n",
    "                        logging.error(f\"Scan CRITICAL: Skipping group '{group_name}': Missing dataset '{ACTION_BITS_KEY}'.\")\n",
    "                        logging.error(f\"Ensure your parser saves the 13-bit card representation as '{ACTION_BITS_KEY}'.\")\n",
    "                        continue # Skip group if critical action data is missing\n",
    "\n",
    "                    # Load data for *this group only* - potential memory use here,\n",
    "                    # but much less than loading ALL groups at once.\n",
    "                    obs_group_data = group[OBS_KEY][:] # Shape (N, INPUT_DIM) bool\n",
    "                    actions_bits_group_data = group[ACTION_BITS_KEY][:] # Shape (N, CARD_BITS) bool\n",
    "\n",
    "                    # --- Basic Validation ---\n",
    "                    if obs_group_data.shape[0] != actions_bits_group_data.shape[0]:\n",
    "                        logging.warning(f\"Scan Skipping group '{group_name}': Data length mismatch ({obs_group_data.shape[0]} vs {actions_bits_group_data.shape[0]}).\")\n",
    "                        continue\n",
    "                    if obs_group_data.shape[0] == 0:\n",
    "                         logging.info(f\"Scan Skipping empty group '{group_name}'.\")\n",
    "                         continue\n",
    "                    if obs_group_data.shape[1] != INPUT_DIM:\n",
    "                        logging.warning(f\"Scan Skipping group '{group_name}': Observation dimension mismatch (Expected {INPUT_DIM}, Got {obs_group_data.shape[1]}).\")\n",
    "                        continue\n",
    "                    if actions_bits_group_data.shape[1] != CARD_BITS:\n",
    "                        logging.warning(f\"Scan Skipping group '{group_name}': Action bits dimension mismatch (Expected {CARD_BITS}, Got {actions_bits_group_data.shape[1]}).\")\n",
    "                        continue\n",
    "\n",
    "                    # --- Convert Action Card Bits to Action Index (0-8) ---\n",
    "                    # Iterate through each sample in the group\n",
    "                    # Convert boolean arrays to integers (0 or 1) for comparison\n",
    "                    obs_group_int = obs_group_data.astype(np.int8)\n",
    "                    actions_bits_group_int = actions_bits_group_data.astype(np.int8)\n",
    "\n",
    "                    for j in range(obs_group_int.shape[0]):\n",
    "                        state_vec_int = obs_group_int[j, :]\n",
    "                        played_card_bits_int = actions_bits_group_int[j, :]\n",
    "\n",
    "                        # Extract the player's hand bits from the state vector\n",
    "                        hand_bits_int = state_vec_int[HAND_START_BIT_INDEX : HAND_START_BIT_INDEX + NUM_CARDS_HAND * CARD_BITS]\n",
    "                        # Reshape hand bits for easier card-by-card comparison\n",
    "                        hand_bits_reshaped = hand_bits_int.reshape(NUM_CARDS_HAND, CARD_BITS) # Shape (9, 13)\n",
    "\n",
    "                        # Find the index of the played_card_bits within the hand_bits\n",
    "                        action_index = -1\n",
    "                        # Use axis=1 for comparison along the 13-bit dimension\n",
    "                        # np.all checks if all elements in the comparison result are True\n",
    "                        # np.where finds the indices where this condition is met\n",
    "                        matches = np.where(np.all(hand_bits_reshaped == played_card_bits_int, axis=1))[0]\n",
    "\n",
    "                        if len(matches) == 1:\n",
    "                            # Found exactly one matching card in hand\n",
    "                            action_index = matches[0]\n",
    "                            valid_samples.append((group_name, j, int(action_index))) # Store as tuple (group, index_in_group, derived_action_index)\n",
    "                        elif len(matches) > 1:\n",
    "                             # This should ideally not happen if hand logic is correct and cards are unique\n",
    "                             logging.warning(f\"Scan WARNING: Found multiple matches ({len(matches)}) for played card (bits: {played_card_bits_int.tolist()}) in hand for sample {j} in group '{group_name}'. Skipping this sample.\")\n",
    "                             # Decide how to handle: skip, pick first, etc. Skipping is safest.\n",
    "                             continue # Skip this sample\n",
    "                        else: # len(matches) == 0\n",
    "                             # This indicates the card played was not found in the state's hand representation\n",
    "                             # (e.g., parsing error, state/action mismatch)\n",
    "                             # logging.warning(f\"Scan WARNING: Card played (bits: {played_card_bits_int.tolist()}) not found in hand for sample {j} in group '{group_name}'. Skipping this sample.\")\n",
    "                             continue # Skip this sample as action index is ambiguous/invalid\n",
    "\n",
    "                    # Free up memory from the loaded group data before processing next group\n",
    "                    del obs_group_data, actions_bits_group_data, obs_group_int, actions_bits_group_int, hand_bits_int, hand_bits_reshaped\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Scan Error processing group '{group_name}': {e}\")\n",
    "                    traceback.print_exc()\n",
    "                    continue # Skip this group on error\n",
    "\n",
    "        logging.info(f\"Scan complete. Identified {len(valid_samples)} valid samples across all groups.\")\n",
    "        return valid_samples\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Scan: Failed to open or process HDF5 file '{filepath}': {e}\")\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"--- Starting Card Policy Training (BC) ---\")\n",
    "\n",
    "    # --- 1. Scan HDF5 and get list of valid samples ---\n",
    "    logging.info(f\"Scanning HDF5 file '{HDF5_FILE_PATH}' to identify valid samples...\")\n",
    "    all_valid_samples = scan_hdf5_for_valid_samples(HDF5_FILE_PATH)\n",
    "\n",
    "    if not all_valid_samples:\n",
    "        logging.critical(\"No valid samples found in the HDF5 file after scanning. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    logging.info(f\"Total valid data samples identified: {len(all_valid_samples)}.\")\n",
    "\n",
    "    # --- 2. Split the list of valid samples ---\n",
    "    # Split the list of (group_name, index_in_group, derived_action_index) tuples\n",
    "    logging.info(f\"Splitting valid samples (keeping {1-VALIDATION_SPLIT_SIZE:.0%} for training)...\")\n",
    "    train_samples_list, val_samples_list = train_test_split(\n",
    "        all_valid_samples,\n",
    "        test_size=VALIDATION_SPLIT_SIZE,\n",
    "        random_state=RANDOM_SEED,\n",
    "        shuffle=True,\n",
    "        # Stratify is tricky here because we only have the index (0-8), not the full action.\n",
    "        # If distribution is heavily skewed, consider a custom stratified split on the index_in_tuple[2].\n",
    "        # For simplicity, skipping stratification for now.\n",
    "    )\n",
    "    logging.info(f\"BC Training samples: {len(train_samples_list)}, Validation samples: {len(val_samples_list)}\")\n",
    "    # Free up memory from the full list\n",
    "    del all_valid_samples, val_samples_list # Keep val_samples_list if needed for evaluation later\n",
    "\n",
    "\n",
    "    # --- 3. Create Datasets and DataLoaders ---\n",
    "    train_dataset = JassCardDataset(HDF5_FILE_PATH, train_samples_list)\n",
    "\n",
    "    # Create DataLoader for batching and shuffling training data\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BC_BATCH_SIZE,\n",
    "        shuffle=True, # Shuffle training data\n",
    "        num_workers=BC_NUM_WORKERS, # 0 means main process, adjust if needed\n",
    "        pin_memory=True if device.type == 'cuda' else False # Pin memory for faster GPU transfer\n",
    "    )\n",
    "    logging.info(f\"Created DataLoader with batch_size={BC_BATCH_SIZE}, num_workers={BC_NUM_WORKERS}.\")\n",
    "    # Free up memory from the list now held by the dataset\n",
    "    del train_samples_list\n",
    "\n",
    "\n",
    "    # --- 4. Define Environment Spaces for Imitation Library ---\n",
    "    observation_space = gym.spaces.Box(low=0, high=1, shape=(INPUT_DIM,), dtype=np.float32)\n",
    "    action_space = gym.spaces.Discrete(CARD_ACTION_DIM) # Discrete space for action index (0-8)\n",
    "\n",
    "\n",
    "    # --- 5. Train Behavioral Cloning Model using DataLoader ---\n",
    "    rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "    # The BC trainer can accept a DataLoader directly via the 'data_loader' argument\n",
    "    # This is the key difference to avoid loading all data into 'demonstrations'.\n",
    "    bc_trainer = bc.BC(\n",
    "        observation_space=observation_space,\n",
    "        action_space=action_space,\n",
    "        batch_size=BC_BATCH_SIZE, # Note: batch_size here is used by BC internally but the DataLoader controls the actual batching\n",
    "        optimizer_kwargs=dict(lr=BC_LEARNING_RATE),\n",
    "        device=device, # Ensure the trainer/policy are created on the correct device\n",
    "        rng=rng,\n",
    "        # Note: BC trainer calculates loss batch by batch from the DataLoader\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Starting BC training for {BC_N_EPOCHS} epochs using DataLoader...\")\n",
    "    if len(train_dataset) > 0:\n",
    "        # When using data_loader, train method uses n_epochs argument directly.\n",
    "        # The loader yields batches, and train runs for the specified number of epochs.\n",
    "        bc_trainer.train(n_epochs=BC_N_EPOCHS, data_loader=train_loader) # <-- ADD data_loader here\n",
    "        # Optional: Add validation evaluation here after training if val_samples_list is kept\n",
    "\n",
    "    else:\n",
    "        logging.warning(\"No training samples available. Skipping training.\")\n",
    "\n",
    "\n",
    "    # --- 6. Save Policy ---\n",
    "    # We assume training finished if we got here without critical errors and train_dataset was not empty\n",
    "    if len(train_dataset) > 0:\n",
    "        try:\n",
    "            # The policy object is stored in bc_trainer.policy\n",
    "            # Save it using the underlying SB3 policy save method\n",
    "            # This saves an SB3-compatible policy, usually as a .zip file\n",
    "            save_path_zip = f\"{POLICY_SAVE_PATH}.zip\"\n",
    "            bc_trainer.policy.save(save_path_zip)\n",
    "            logging.info(f\"Card prediction policy saved successfully to {save_path_zip}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving BC policy: {e}\")\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        logging.warning(\"Skipping policy save because no training was performed.\")\n",
    "\n",
    "\n",
    "    logging.info(\"--- Card Policy Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83298718-7a82-4fa5-bf77-53acc50edfff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
