{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d9275c-acce-4866-8312-7fc0fa3712b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "train_card_policy_bc.py\n",
    "\n",
    "Trains a card prediction policy using Behavioral Cloning (BC)\n",
    "based on data parsed into an HDF5 file.\n",
    "\n",
    "This version uses a class to organize the training process\n",
    "and leverages a custom HDF5 dataset with DataLoader for\n",
    "memory-efficient training on large datasets.\n",
    "\n",
    "Requires compatible versions of torch, gymnasium, stable-baselines3, imitation, and h5py.\n",
    "\"\"\"\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "# stable_baselines3 is implicitly used by imitation for policy structure\n",
    "import stable_baselines3 as sb3\n",
    "# Import core BC algorithm and data types\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import types\n",
    "# Need Dataset and DataLoader for custom data loading\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as data_utils # Use alias for DataLoader\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import torch as th # Use th alias for torch conventions\n",
    "import traceback # To print full error tracebacks\n",
    "from typing import Tuple, Optional, List, Dict, Any\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# --- Logging Configuration ---\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "HDF5_FILE_PATH = r'../../../../Training_Data/jass.hdf5' # <--- Path to the HDF5 file from your parser\n",
    "POLICY_SAVE_PATH = 'jass_bc_card_policy_591_organized' # Model save path\n",
    "\n",
    "# --- Data Keys (Must match your parser's output HDF5 structure) ---\n",
    "OBS_KEY = 'state_bits'         # N x 929 boolean array\n",
    "ACTION_BITS_KEY = 'action_card_bits' # N x 13 boolean array (the played card's representation)\n",
    "\n",
    "# --- Model Dimensions (Based on your parser) ---\n",
    "INPUT_DIM = 929 # From your parser's STATE_BITS calculation\n",
    "CARD_ACTION_DIM = 9 # 9 possible card positions in hand (index 0-8)\n",
    "\n",
    "# --- Constants from Parser (Needed to interpret state_bits) ---\n",
    "CARD_BITS = 13 # Bits used to represent a single card in the parser\n",
    "NUM_CARDS_HISTORY = 32 # Number of history card slots in state (32 * 13 bits)\n",
    "NUM_CARDS_TABLE = 3  # Number of table card slots in state (3 * 13 bits)\n",
    "NUM_CARDS_HAND = 9   # Number of hand card slots in state (9 * 13 bits)\n",
    "\n",
    "# Calculate the starting index of the player's hand bits within the state_bits vector\n",
    "# Order from parser: History, Table, Hand, Shown, Trump\n",
    "HAND_START_BIT_INDEX = (NUM_CARDS_HISTORY * CARD_BITS) + (NUM_CARDS_TABLE * CARD_BITS)\n",
    "# 32 * 13 = 416\n",
    "# 3 * 13  = 39\n",
    "# HAND_START_BIT_INDEX = 416 + 39 = 455. The hand bits are from index 455 up to 455 + (9*13) = 455 + 117 = 572.\n",
    "\n",
    "\n",
    "# --- Default Training Hyperparameters ---\n",
    "# Can be overridden when instantiating the trainer class\n",
    "DEFAULT_TRAINING_PARAMS = {\n",
    "    'validation_split_size': 0.15,\n",
    "    'random_seed': 42,\n",
    "    'bc_batch_size': 32,\n",
    "    'bc_learning_rate': 3e-4,\n",
    "    'bc_n_epochs': 10,\n",
    "    'device': th.device(\"cuda\"),\n",
    "    'dataloader_num_workers': 0, # Start with 0 workers for simplicity, increase for speed\n",
    "    'dataloader_pin_memory': True,\n",
    "}\n",
    "\n",
    "\n",
    "# --- 1. Data Location Collection Helper Function ---\n",
    "# This function remains outside the class as it's a data loading utility\n",
    "def collect_bc_sample_locations_from_hdf5(filepath: str) -> Tuple[Optional[List[Tuple[str, int]]], int]:\n",
    "    \"\"\"\n",
    "    Collects locations (group_name, sample_index) for valid BC samples\n",
    "    from all groups in the HDF5 file. Does NOT load actual data into memory.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Optional[list], int]: A list of (group_name, index_in_group) tuples\n",
    "                                     and the total count of samples, or None and 0 on failure.\n",
    "    \"\"\"\n",
    "    sample_locations = []\n",
    "    total_samples_count = 0\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        logging.error(f\"HDF5 file not found at: {filepath}\")\n",
    "        return None, 0\n",
    "\n",
    "    try:\n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            # Get all group names\n",
    "            # game_groups = list(f.keys()) # Process all groups\n",
    "            #game_groups = list(f.keys()) # Process all groups initially\n",
    "            game_groups = list(f.keys())[:2000] # Example: Process only first 2000 games\n",
    "\n",
    "            logging.info(f\"Found {len(f.keys())} game groups total. Processing {len(game_groups)} groups for locations.\")\n",
    "\n",
    "            if not game_groups:\n",
    "                logging.error(\"No game groups found in the HDF5 file.\")\n",
    "                return None, 0\n",
    "\n",
    "            for i, group_name in enumerate(game_groups):\n",
    "                if (i + 1) % 200 == 0:\n",
    "                    logging.info(f\"Collecting locations from group {i+1}/{len(game_groups)}: {group_name}\")\n",
    "                try:\n",
    "                    group = f[group_name]\n",
    "                    # --- Check required keys (without reading data) ---\n",
    "                    if OBS_KEY not in group:\n",
    "                        logging.warning(f\"Skipping group '{group_name}': Missing dataset '{OBS_KEY}'\")\n",
    "                        continue\n",
    "                    if ACTION_BITS_KEY not in group:\n",
    "                        logging.error(f\"CRITICAL: Skipping group '{group_name}': Missing dataset '{ACTION_BITS_KEY}'.\")\n",
    "                        continue\n",
    "\n",
    "                    # Get dataset sizes and shapes without loading data\n",
    "                    obs_dset_shape = group[OBS_KEY].shape\n",
    "                    actions_bits_dset_shape = group[ACTION_BITS_KEY].shape\n",
    "\n",
    "                    # --- Basic Validation (shape only) ---\n",
    "                    if obs_dset_shape[0] != actions_bits_dset_shape[0]:\n",
    "                         logging.warning(f\"Skipping group '{group_name}': Data length mismatch ({obs_dset_shape[0]} vs {actions_bits_dset_shape[0]}).\")\n",
    "                         continue\n",
    "                    if obs_dset_shape[0] == 0:\n",
    "                         logging.info(f\"Skipping empty group '{group_name}'.\")\n",
    "                         continue\n",
    "                    if obs_dset_shape[1] != INPUT_DIM:\n",
    "                         logging.warning(f\"Skipping group '{group_name}': Observation dimension mismatch (Expected {INPUT_DIM}, Got {obs_dset_shape[1]}).\")\n",
    "                         continue\n",
    "                    if actions_bits_dset_shape[1] != CARD_BITS:\n",
    "                         logging.warning(f\"Skipping group '{group_name}': Action bits dimension mismatch (Expected {CARD_BITS}, Got {actions_bits_dset_shape[1]}).\")\n",
    "                         continue\n",
    "\n",
    "                    # Collect locations for all samples in this group that passed initial checks.\n",
    "                    # The action index derivation (card in hand check) will happen in the Dataset.\n",
    "                    for j in range(obs_dset_shape[0]):\n",
    "                        sample_locations.append((group_name, j))\n",
    "\n",
    "                    total_samples_count += obs_dset_shape[0] # Count all samples from valid groups\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing group '{group_name}' for locations: {e}\")\n",
    "                    traceback.print_exc()\n",
    "                    continue # Skip this group on error\n",
    "\n",
    "            logging.info(f\"Finished collecting locations for {len(game_groups)} groups. Total potential samples found: {len(sample_locations)}\")\n",
    "\n",
    "            return sample_locations, total_samples_count\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Failed to access HDF5 file '{filepath}' or process groups for locations: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None, 0\n",
    "\n",
    "\n",
    "# --- 2. Custom HDF5 Dataset Class ---\n",
    "# This class also remains outside the trainer class as it's a generic data interface\n",
    "class HDF5Dataset(Dataset):\n",
    "    \"\"\"Custom Dataset for reading observations and converting action bits\n",
    "       to indices from HDF5 file on the fly.\"\"\"\n",
    "\n",
    "    def __init__(self, hdf5_filepath: str, sample_locations: List[Tuple[str, int]]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hdf5_filepath (str): Path to the HDF5 file.\n",
    "            sample_locations (list): List of (group_name, index_in_group) tuples\n",
    "                                     representing the samples in this dataset split.\n",
    "        \"\"\"\n",
    "        self.hdf5_filepath = hdf5_filepath\n",
    "        self.sample_locations = sample_locations\n",
    "        self._file = None # HDF5 file handle, opened on first access\n",
    "\n",
    "        # Basic validation of locations structure (optional but good)\n",
    "        if not all(isinstance(loc, tuple) and len(loc) == 2 for loc in sample_locations):\n",
    "             logging.warning(\"Dataset: Sample locations list has unexpected format.\")\n",
    "        if not sample_locations:\n",
    "             logging.warning(\"Dataset: Initialized with an empty list of sample locations.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in this dataset split.\"\"\"\n",
    "        return len(self.sample_locations)\n",
    "\n",
    "    def _get_file(self):\n",
    "        \"\"\"Helper to open the HDF5 file. Designed to be safer with multiprocessing\n",
    "           by potentially opening a handle per worker process used by DataLoader.\"\"\"\n",
    "        # Check if the file is already open in this process/thread\n",
    "        # h5py File objects are not thread-safe. Opening per process is safer.\n",
    "        # A simple check for self._file being None works well with DataLoader's\n",
    "        # worker initialization if the dataset object is copied to workers.\n",
    "        if self._file is None:\n",
    "             try:\n",
    "                 # Using swmr=True (Single Writer Multiple Reader) might be necessary\n",
    "                 # if the HDF5 file could potentially be written to while reading,\n",
    "                 # but for a static file, 'r' mode is fine. swmr requires HDF5 1.9+\n",
    "                 # self._file = h5py.File(self.hdf5_filepath, 'r', swmr=True)\n",
    "                 self._file = h5py.File(self.hdf5_filepath, 'r')\n",
    "                 # logging.debug(f\"Opened HDF5 file {self.hdf5_filepath} in process {os.getpid()}\") # Optional: log file opening\n",
    "             except Exception as e:\n",
    "                  logging.critical(f\"Dataset Error: Failed to open HDF5 file {self.hdf5_filepath} in _get_file for process {os.getpid()}: {e}\")\n",
    "                  # In a real scenario, failing to open the file is critical.\n",
    "                  # Raising an error will stop the DataLoader worker or main process.\n",
    "                  raise\n",
    "        return self._file\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Ensures the HDF5 file is closed when the dataset object is garbage collected.\"\"\"\n",
    "        if self._file is not None:\n",
    "             try:\n",
    "                 self._file.close()\n",
    "                 # logging.debug(f\"Closed HDF5 file {self.hdf5_filepath} in __del__ for process {os.getpid()}\") # Optional: log file closing\n",
    "             except Exception as e:\n",
    "                 logging.error(f\"Dataset Error: Error closing HDF5 file {self.hdf5_filepath} in __del__: {e}\")\n",
    "             self._file = None # Clear the reference\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Reads a single sample from the HDF5 file and returns observation and action index.\n",
    "        Performs the action bit to index conversion.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve (from 0 to len - 1).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[th.Tensor, th.Tensor]: The observation (float32) and\n",
    "                                         the action index (int64).\n",
    "\n",
    "        Raises:\n",
    "            IndexError: If idx is out of the valid range.\n",
    "            RuntimeError: If data inconsistencies prevent deriving a valid action index.\n",
    "            Exception: For other errors during file reading.\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= len(self.sample_locations):\n",
    "            raise IndexError(f\"Dataset index ({idx}) out of range (0-{len(self.sample_locations)-1})\")\n",
    "\n",
    "        group_name, sample_index_in_group = self.sample_locations[idx]\n",
    "\n",
    "        # Use the helper to get the file object (opens if not already open in this process)\n",
    "        f = self._get_file()\n",
    "\n",
    "        try:\n",
    "            # Access the group and datasets\n",
    "            group = f[group_name]\n",
    "            # Using [index:index+1] slicing reads a single row and keeps the dimension (shape (1, D))\n",
    "            obs_data_single = group[OBS_KEY][sample_index_in_group:sample_index_in_group+1] # Shape (1, INPUT_DIM) bool\n",
    "            actions_bits_data_single = group[ACTION_BITS_KEY][sample_index_in_group:sample_index_in_group+1] # Shape (1, CARD_BITS) bool\n",
    "\n",
    "            # --- Convert Action Card Bits (1, 13) bool to Action Index (scalar) int64 ---\n",
    "            # Convert boolean arrays to integers for easier comparison/processing\n",
    "            state_vec_int_single = obs_data_single.astype(np.int8).squeeze(axis=0) # Shape (INPUT_DIM,) int8\n",
    "            played_card_bits_int_single = actions_bits_data_single.astype(np.int8).squeeze(axis=0) # Shape (CARD_BITS,) int8\n",
    "\n",
    "            # Initialize action_index BEFORE the loop and BEFORE the check below\n",
    "            action_index = -1\n",
    "\n",
    "            # Extract the player's hand bits from the single state vector\n",
    "            hand_bits_int_single = state_vec_int_single[HAND_START_BIT_INDEX : HAND_START_BIT_INDEX + NUM_CARDS_HAND * CARD_BITS]\n",
    "\n",
    "            # Find the index of the played_card_bits within the hand_bits\n",
    "            for card_idx_in_hand in range(NUM_CARDS_HAND):\n",
    "                start_idx = card_idx_in_hand * CARD_BITS\n",
    "                end_idx = start_idx + CARD_BITS\n",
    "                current_hand_card_bits_int = hand_bits_int_single[start_idx : end_idx]\n",
    "\n",
    "                # Compare the played card bits with the current hand card bits\n",
    "                if np.array_equal(played_card_bits_int_single, current_hand_card_bits_int):\n",
    "                    action_index = card_idx_in_hand # Found the index!\n",
    "                    break # Found the card in hand, its index is our action\n",
    "\n",
    "            # --- Error Handling for Data Inconsistency ---\n",
    "            # This check happens AFTER the loop finishes and action_index has been determined (or remained -1).\n",
    "            if action_index == -1:\n",
    "                 played_bits_list = played_card_bits_int_single.tolist()\n",
    "                 hand_bits_list = hand_bits_int_single.reshape(-1, CARD_BITS).tolist()\n",
    "                 error_msg = (f\"Dataset Error: Sample {idx} ({group_name}, {sample_index_in_group}): \"\n",
    "                              f\"Played card bits {played_bits_list} not found in hand bits {hand_bits_list}. \"\n",
    "                              \"This sample cannot be used for training.\")\n",
    "                 # Log the error for debugging data issues\n",
    "                 logging.error(error_msg)\n",
    "                 # Raise a RuntimeError to stop training for this data problem\n",
    "                 raise RuntimeError(error_msg)\n",
    "\n",
    "\n",
    "            # Convert numpy arrays to torch tensors - THIS IS *AFTER* the error check\n",
    "            # Squeeze removes the leading dimension of 1 from the [index:index+1] slice\n",
    "            obs_tensor = th.from_numpy(obs_data_single.astype(np.float32)).squeeze(axis=0) # Shape (INPUT_DIM,) float32\n",
    "            action_index_tensor = th.tensor(action_index, dtype=th.int64) # Scalar tensor (int64)\n",
    "\n",
    "            # Return a dictionary expected by the DataLoader/imitation trainer\n",
    "            return {\"obs\": obs_tensor, \"acts\": action_index_tensor}\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch any exception occurring during reading/processing this specific sample\n",
    "            logging.error(f\"Dataset Error: Error reading or processing sample {idx} ({group_name}, {sample_index_in_group}) in __getitem__: {e}\")\n",
    "            traceback.print_exc()\n",
    "            # Re-raise the exception to signal the error to the DataLoader/trainer\n",
    "            raise\n",
    "\n",
    "\n",
    "# --- 3. Trainer Class ---\n",
    "# This class encapsulates the training setup and execution\n",
    "class BCCardPolicyTrainer:\n",
    "    \"\"\"\n",
    "    Encapsulates the Behavioral Cloning training process for the Jass card policy.\n",
    "    Handles data loading setup, trainer initialization, training, and saving.\n",
    "    \"\"\"\n",
    "    def __init__(self, hdf5_filepath: str, save_path: str, params: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initializes the trainer.\n",
    "\n",
    "        Args:\n",
    "            hdf5_filepath (str): Path to the HDF5 file containing training data.\n",
    "            save_path (str): Base path for saving the trained policy.\n",
    "            params (Dict[str, Any]): Dictionary of training parameters.\n",
    "        \"\"\"\n",
    "        self.hdf5_filepath = hdf5_filepath\n",
    "        self.save_path = save_path\n",
    "        self.params = params # Store parameters\n",
    "\n",
    "        # Apply default parameters if not provided\n",
    "        for key, default_value in DEFAULT_TRAINING_PARAMS.items():\n",
    "            if key not in self.params:\n",
    "                self.params[key] = default_value\n",
    "\n",
    "        # Extract parameters for clarity\n",
    "        self.validation_split_size = self.params['validation_split_size']\n",
    "        self.random_seed = self.params['random_seed']\n",
    "        self.bc_batch_size = self.params['bc_batch_size']\n",
    "        self.bc_learning_rate = self.params['bc_learning_rate']\n",
    "        self.bc_n_epochs = self.params['bc_n_epochs']\n",
    "        self.device = self.params['device']\n",
    "        self.dataloader_num_workers = self.params['dataloader_num_workers']\n",
    "        self.dataloader_pin_memory = self.params['dataloader_pin_memory']\n",
    "\n",
    "        logging.info(f\"Trainer initialized with parameters: {self.params}\")\n",
    "        logging.info(f\"Using device for training: {self.device}\")\n",
    "\n",
    "\n",
    "        self.train_dataloader = None\n",
    "        # self.val_dataloader = None # Optional: for validation during training\n",
    "        self.bc_trainer = None\n",
    "        self.n_batches = 0 # Total training batches calculated later\n",
    "\n",
    "\n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"\n",
    "        Collects sample locations, splits them, and creates HDF5Dataset and DataLoaders.\n",
    "        \"\"\"\n",
    "        logging.info(\"Loading and preparing data...\")\n",
    "\n",
    "        # --- Collect Data Sample Locations ---\n",
    "        sample_locations, total_samples = collect_bc_sample_locations_from_hdf5(self.hdf5_filepath)\n",
    "\n",
    "        # --- Error Handling for Location Collection ---\n",
    "        if sample_locations is None:\n",
    "            logging.critical(\"Trainer Error: Failed to collect sample locations (function returned None). Check HDF5_FILE_PATH and file accessibility. Exiting.\")\n",
    "            return False # Indicate failure\n",
    "\n",
    "        if not sample_locations:\n",
    "             logging.critical(\"Trainer Error: No valid data sample locations were collected (list is empty). Ensure HDF5 file contains groups with required datasets and correct dimensions. Exiting.\")\n",
    "             return False # Indicate failure\n",
    "\n",
    "        logging.info(f\"Total valid data sample locations collected: {len(sample_locations)}.\")\n",
    "\n",
    "        # --- Split Sample Locations ---\n",
    "        logging.info(f\"Splitting sample locations (keeping {1-self.validation_split_size:.0%} for training)...\")\n",
    "        try:\n",
    "            train_locations, val_locations = train_test_split(\n",
    "                sample_locations,\n",
    "                test_size=self.validation_split_size,\n",
    "                random_state=self.random_seed,\n",
    "                shuffle=True,\n",
    "            )\n",
    "        except ValueError as e:\n",
    "             logging.critical(f\"Trainer Error: Failed to split data locations. This can happen if test_size is too large for the number of samples. Error: {e}. Exiting.\")\n",
    "             return False # Indicate failure\n",
    "\n",
    "        logging.info(f\"Training locations: {len(train_locations)}, Validation locations: {len(val_locations)}\")\n",
    "\n",
    "        # Free up memory from the original full locations list\n",
    "        del sample_locations\n",
    "\n",
    "        # --- Create HDF5 Dataset for Training ---\n",
    "        if not train_locations:\n",
    "            logging.critical(\"Trainer Error: No training samples available after splitting locations. Adjust split size or provide more data. Exiting.\")\n",
    "            return False # Indicate failure\n",
    "        train_dataset = HDF5Dataset(self.hdf5_filepath, train_locations)\n",
    "\n",
    "        # --- Create DataLoader from the Training Dataset ---\n",
    "        # This DataLoader will be used by the BC trainer\n",
    "        self.train_dataloader = data_utils.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.bc_batch_size,\n",
    "            shuffle=True, # Shuffle data for better training convergence\n",
    "            num_workers=self.dataloader_num_workers,\n",
    "            pin_memory=self.dataloader_pin_memory,\n",
    "        )\n",
    "        logging.info(f\"Created training DataLoader with batch_size={self.bc_batch_size}, num_workers={self.dataloader_num_workers}\")\n",
    "\n",
    "\n",
    "        # --- Optional: Create DataLoader for Validation ---\n",
    "        # You would use this if you configure the BC trainer for validation during training\n",
    "        # if val_locations:\n",
    "        #     self.val_dataloader = data_utils.DataLoader(\n",
    "        #         HDF5Dataset(self.hdf5_filepath, val_locations),\n",
    "        #         batch_size=self.bc_batch_size, # Validation batch size can be different\n",
    "        #         shuffle=False, # No need to shuffle validation data\n",
    "        #         num_workers=self.dataloader_num_workers,\n",
    "        #         pin_memory=self.dataloader_pin_memory,\n",
    "        #     )\n",
    "        #     logging.info(f\"Created validation DataLoader with batch_size={self.bc_batch_size}, num_workers={self.dataloader_num_workers}\")\n",
    "        # else:\n",
    "        #     logging.warning(\"No validation samples available after splitting locations. Skipping validation DataLoader creation.\")\n",
    "        #     self.val_dataloader = None\n",
    "\n",
    "\n",
    "        # Calculate total number of training batches needed\n",
    "        num_train_samples = len(train_dataset)\n",
    "        n_batches_per_epoch = num_train_samples // self.bc_batch_size\n",
    "        if num_train_samples % self.bc_batch_size != 0:\n",
    "            n_batches_per_epoch += 1\n",
    "        self.n_batches = n_batches_per_epoch * self.bc_n_epochs\n",
    "        # Ensure n_batches is at least 1 if there are samples\n",
    "        self.n_batches = max(1, self.n_batches) if num_train_samples > 0 else 0\n",
    "\n",
    "        logging.info(f\"Data preparation complete. Total training batches calculated: {self.n_batches}\")\n",
    "        return True # Indicate success\n",
    "\n",
    "\n",
    "    def setup_trainer(self):\n",
    "        \"\"\"Initializes the imitation.algorithms.bc.BC trainer.\"\"\"\n",
    "        if self.train_dataloader is None:\n",
    "            logging.critical(\"Trainer Error: Data not loaded. Call load_and_prepare_data() first. Exiting.\")\n",
    "            return False # Indicate failure\n",
    "\n",
    "        logging.info(\"Setting up BC trainer...\")\n",
    "\n",
    "        # --- Prepare Spaces for Imitation Library ---\n",
    "        # These match the expected output of HDF5Dataset.__getitem__ and DataLoader collation\n",
    "        observation_space = gym.spaces.Box(low=0, high=1, shape=(INPUT_DIM,), dtype=np.float32)\n",
    "        action_space = gym.spaces.Discrete(CARD_ACTION_DIM) # Discrete(9) for action index\n",
    "\n",
    "        # --- Initialize Behavioral Cloning Model ---\n",
    "        rng = np.random.default_rng(self.random_seed)\n",
    "\n",
    "        try:\n",
    "            self.bc_trainer = bc.BC(\n",
    "                observation_space=observation_space,\n",
    "                action_space=action_space,\n",
    "                demonstrations=self.train_dataloader, # Pass the DataLoader instance\n",
    "                batch_size=self.bc_batch_size, # Passed for internal logic/logging? DataLoader controls actual batch size\n",
    "                optimizer_kwargs=dict(lr=self.bc_learning_rate),\n",
    "                device=self.device, # Trainer and policy will be moved to this device\n",
    "                rng=rng,\n",
    "                # Note: loss_calculator is handled internally by bc.BC using Cross-Entropy Loss for Discrete actions.\n",
    "                # eval_dataloader=self.val_dataloader, # Pass validation loader if created\n",
    "                # eval_metrics=..., # Define appropriate metrics if evaluating\n",
    "            )\n",
    "            logging.info(\"BC trainer initialized successfully.\")\n",
    "            return True # Indicate success\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Trainer Error: Failed to initialize BC trainer: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return False # Indicate failure\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Runs the Behavioral Cloning training process.\"\"\"\n",
    "        if self.bc_trainer is None:\n",
    "            logging.critical(\"Trainer Error: Trainer not set up. Call setup_trainer() first. Exiting.\")\n",
    "            return False # Indicate failure\n",
    "\n",
    "        if self.n_batches == 0:\n",
    "            logging.warning(\"Trainer: No training batches calculated. Skipping training.\")\n",
    "            return True # Consider success if no training needed due to lack of data\n",
    "\n",
    "        logging.info(f\"Starting BC training for {self.bc_n_epochs} epochs totaling {self.n_batches} batches...\")\n",
    "\n",
    "        try:\n",
    "            # bc.BC.train takes total batches to run for.\n",
    "            # It iterates through the provided DataLoader to get batches.\n",
    "            log_interval = max(1, self.n_batches // 50) # Log ~50 times during training\n",
    "            logging.info(f\"Logging training progress every {log_interval} batches.\")\n",
    "            self.bc_trainer.train(n_batches=self.n_batches, log_interval=log_interval)\n",
    "\n",
    "            logging.info(\"BC training finished.\")\n",
    "            return True # Indicate success\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Trainer Error: An error occurred during training: {e}\")\n",
    "            traceback.print_exc()\n",
    "            # The device mismatch error would likely happen here\n",
    "            return False # Indicate failure\n",
    "\n",
    "\n",
    "    def save_policy(self):\n",
    "        \"\"\"Saves the trained policy model.\"\"\"\n",
    "        if self.bc_trainer is None or self.bc_trainer.policy is None:\n",
    "            logging.warning(\"Trainer: No policy to save. Training may have failed or was skipped.\")\n",
    "            return False # Indicate failure\n",
    "\n",
    "        if self.n_batches == 0: # Only attempt saving if training was intended\n",
    "             logging.warning(\"Trainer: Skipping policy save because no training batches were available.\")\n",
    "             return True # Consider success if saving was skipped intentionally\n",
    "\n",
    "        logging.info(f\"Attempting to save policy to {self.save_path}.zip\")\n",
    "        try:\n",
    "            # The policy object is stored in bc_trainer.policy.\n",
    "            # Save it using the underlying SB3 policy save method (.zip file).\n",
    "            save_path_full = f\"{self.save_path}.zip\"\n",
    "            self.bc_trainer.policy.save(save_path_full)\n",
    "            logging.info(f\"Policy saved successfully to {save_path_full}\")\n",
    "            return True # Indicate success\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Trainer Error: Error saving BC policy: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return False # Indicate failure\n",
    "\n",
    "\n",
    "    # Optional: Add a load_policy method here if needed for inference later\n",
    "    # def load_policy(self, model_path: str):\n",
    "    #     \"\"\"Loads a trained policy model.\"\"\"\n",
    "    #     try:\n",
    "    #         # Assuming it's an SB3 policy saved with .save()\n",
    "    #         self.policy = sb3.common.policies.deserialize_policy(model_path, device=self.device)\n",
    "    #         logging.info(f\"Policy loaded successfully from {model_path}\")\n",
    "    #         # You might need observation_space and action_space defined if loading policy standalone\n",
    "    #     except Exception as e:\n",
    "    #         logging.error(f\"Error loading policy from {model_path}: {e}\")\n",
    "    #         self.policy = None\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"--- Script Start ---\")\n",
    "\n",
    "    # --- Configure and Instantiate Trainer ---\n",
    "    # Use default parameters, or override specific ones here\n",
    "    trainer_params = DEFAULT_TRAINING_PARAMS.copy()\n",
    "    # Example override:\n",
    "    # trainer_params['bc_n_epochs'] = 20\n",
    "    # trainer_params['dataloader_num_workers'] = 4 # Increase workers for speed\n",
    "    # trainer_params['device'] = th.device(\"cuda:1\") # Specify a different GPU if available\n",
    "\n",
    "    logging.info(f\"Using device: {trainer_params['device']}\")\n",
    "\n",
    "\n",
    "    trainer = BCCardPolicyTrainer(\n",
    "        hdf5_filepath=HDF5_FILE_PATH,\n",
    "        save_path=POLICY_SAVE_PATH,\n",
    "        params=trainer_params # Pass configuration parameters\n",
    "    )\n",
    "\n",
    "    # --- Execute Training Workflow ---\n",
    "    # 1. Load and prepare data\n",
    "    if not trainer.load_and_prepare_data():\n",
    "        logging.critical(\"Data preparation failed. Exiting.\")\n",
    "        exit() # Exit if data loading/preparation failed\n",
    "\n",
    "    # 2. Setup the BC trainer model\n",
    "    if not trainer.setup_trainer():\n",
    "         logging.critical(\"Trainer setup failed. Exiting.\")\n",
    "         exit() # Exit if trainer initialization failed\n",
    "\n",
    "    # 3. Run the training process\n",
    "    # The device mismatch error would likely happen inside trainer.train()\n",
    "    if not trainer.train():\n",
    "         logging.critical(\"Training process failed. Exiting.\")\n",
    "         exit() # Exit if training encountered a critical error\n",
    "\n",
    "    # 4. Save the trained policy\n",
    "    if not trainer.save_policy():\n",
    "         logging.error(\"Policy saving failed.\")\n",
    "         # Do not necessarily exit, training might have succeeded but saving failed\n",
    "\n",
    "\n",
    "    logging.info(\"--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847d3eea-a123-4af3-a1ac-ff6b32e0aadb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
