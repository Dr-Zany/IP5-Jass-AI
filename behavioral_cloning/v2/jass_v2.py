# -*- coding: utf-8 -*-
"""
train_card_policy_bc.py

Trains a card prediction policy using Behavioral Cloning (BC)
based on data parsed into an HDF5 file.

Assumes the HDF5 file contains 'state_bits' (bool, N x 929) and
'action_card_bits' (bool, N x 13) datasets, as generated by the parser.
Converts action_card_bits to action_index (int, N) during loading.

Uses the standard imitation.algorithms.bc.BC trainer, configured to use
a custom iterator for lazy data loading to improve memory efficiency.
Requires compatible versions of torch, gymnasium, stable-baselines3, and imitation.
"""

import h5py
import numpy as np
import gymnasium as gym
# stable_baselines3 is implicitly used by imitation for policy structure
import stable_baselines3 as sb3
# Import core BC algorithm and data types
from imitation.algorithms import bc
from imitation.data import types
# No need for TrainingMetrics or util imports here when using default loss_calculator

import logging
import os
import torch as th # Use th alias for torch conventions
import traceback # To print full error tracebacks
from typing import Tuple, Optional, List, Iterator
from sklearn.model_selection import train_test_split
import math # For ceiling division


# --- Logging Configuration ---
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')


# --- Configuration ---
HDF5_FILE_PATH = r'../../../Training_Data/jass.hdf5'  # <--- Path to the HDF5 file from your parser
POLICY_SAVE_PATH = 'jass_bc_card_policy_memory_efficient' # Model save path

# --- Data Keys (Must match your parser's output HDF5 structure) ---
OBS_KEY = 'state_bits'        # N x 929 boolean array
ACTION_BITS_KEY = 'action_card_bits' # N x 13 boolean array (the played card's representation)

# --- Model Dimensions (Based on your parser) ---
INPUT_DIM = 929 # From your parser's STATE_BITS calculation
CARD_ACTION_DIM = 9 # 9 possible card positions in hand (index 0-8)

# --- Constants from Parser (Needed to interpret state_bits) ---
CARD_BITS = 13 # Bits used to represent a single card in the parser
NUM_CARDS_HISTORY = 32 # Number of history card slots in state (32 * 13 bits)
NUM_CARDS_TABLE = 3  # Number of table card slots in state (3 * 13 bits)
NUM_CARDS_HAND = 9   # Number of hand card slots in state (9 * 13 bits) - matches CARD_ACTION_DIM conceptually

# Calculate the starting index of the player's hand bits within the state_bits vector
# Order from parser: History, Table, Hand, Shown, Trump
HAND_START_BIT_INDEX = (NUM_CARDS_HISTORY * CARD_BITS) + (NUM_CARDS_TABLE * CARD_BITS)
# 32 * 13 = 416
# 3 * 13  = 39
# HAND_START_BIT_INDEX = 416 + 39 = 455. The hand bits are from index 455 up to 455 + (9*13) = 455 + 117 = 572.

# --- Training Hyperparameters ---
VALIDATION_SPLIT_SIZE = 0.15 # Use 15% of data for validation (optional for BC eval)
RANDOM_SEED = 42 # For reproducible train/test splits
BC_BATCH_SIZE = 32
BC_LEARNING_RATE = 3e-4
BC_N_EPOCHS = 10 # Number of passes over the training data

# Determine device
device = th.device("cuda" if th.cuda.is_available() else "cpu")
logging.info(f"Using device: {device}")


# --- Lazy Loading Iterator ---
class HDF5CardDemoIterator:
    """
    An iterator that reads game state-action pairs from specified HDF5 groups
    and yields them as imitation.data.types.Trajectory objects (length 1).
    Performs action index derivation and filtering on the fly.
    """
    def __init__(self, hdf5_filepath: str, group_names: List[str]):
        self.hdf5_filepath = hdf5_filepath
        self.group_names = group_names
        self._file: Optional[h5py.File] = None
        self._current_group_idx = 0
        self._current_sample_idx = 0
        self._current_obs_data: Optional[np.ndarray] = None
        self._current_actions_bits_data: Optional[np.ndarray] = None

    def __iter__(self):
        """Opens the HDF5 file and resets the iterator state."""
        # Close any previous file handle before opening a new one
        if self._file is not None:
            self.close()
        try:
            # Open file in read mode
            self._file = h5py.File(self.hdf5_filepath, 'r')
        except Exception as e:
             logging.error(f"Failed to open HDF5 file {self.hdf5_filepath}: {e}")
             self._file = None # Ensure file is None on failure
             raise StopIteration # Cannot iterate if file fails to open

        self._current_group_idx = 0
        self._current_sample_idx = 0
        self._current_obs_data = None
        self._current_actions_bits_data = None
        # Pre-load data for the first group if available
        self._load_next_group_data()
        return self

    def __next__(self) -> types.Trajectory:
        """
        Fetches the next valid state-action pair as a Trajectory.
        Handles iterating through samples and groups, deriving action index,
        and skipping invalid samples.
        """
        if self._file is None:
             # Cannot iterate if file wasn't opened successfully
             raise StopIteration

        while True: # Loop to find the next valid sample or raise StopIteration
            if self._current_obs_data is None or self._current_sample_idx >= self._current_obs_data.shape[0]:
                # No more samples in the current group, move to the next group
                self._current_group_idx += 1
                self._current_sample_idx = 0
                self._load_next_group_data()

            # If after trying to load the next group, we still have no data,
            # it means we've processed all groups.
            if self._current_obs_data is None:
                self.close() # Ensure file is closed
                raise StopIteration

            # Process the current sample from the current group
            state_vec_int = self._current_obs_data[self._current_sample_idx, :].astype(np.int8)
            actions_bits_group_int = self._current_actions_bits_data[self._current_sample_idx, :].astype(np.int8)
            played_card_bits_int = actions_bits_group_int # Alias for clarity

            # Store the index *before* incrementing, for retrieving the obs data later
            current_sample_index_in_group = self._current_sample_idx

            # Increment sample index for the next call of __next__
            self._current_sample_idx += 1

            # Extract the player's hand bits from the state vector
            hand_bits_int = state_vec_int[HAND_START_BIT_INDEX : HAND_START_BIT_INDEX + NUM_CARDS_HAND * CARD_BITS]

            # Find the index of the played_card_bits within the hand_bits
            action_index = -1
            for card_idx_in_hand in range(NUM_CARDS_HAND):
                start_idx = card_idx_in_hand * CARD_BITS
                end_idx = start_idx + CARD_BITS
                current_hand_card_bits_int = hand_bits_int[start_idx : end_idx]

                if np.array_equal(played_card_bits_int, current_hand_card_bits_int):
                    action_index = card_idx_in_hand
                    break

            if action_index != -1:
                # Found a valid action index, yield this sample
                # Get the observation data for the sample *we just processed* (using stored index)
                obs_float32 = self._current_obs_data[current_sample_index_in_group : current_sample_index_in_group + 1, :].astype(np.float32)
                action_int64 = np.array([action_index], dtype=np.int64)

                # Return as a Trajectory of length 1
                # Corrected: Removed 'next_obs' and replaced 'dones' with 'terminal'
                return types.Trajectory(
                    obs=obs_float32, # Shape (1, INPUT_DIM) float32
                    acts=action_int64, # Shape (1,) int64
                    infos=np.array([None], dtype=object), # Shape (1,)
                    terminal=False # Assuming these state-action pairs are not terminal steps in an episode context for BC
                )
            else:
                # Invalid sample (card not found in hand), continue loop to find the next one
                # The sample index was already incremented above, so next __next__ will try the next sample
                pass # Loop continues to the next sample or group

    def _load_next_group_data(self):
        """Helper to load data from the next group that has the required datasets."""
        self._current_obs_data = None
        self._current_actions_bits_data = None

        if self._file is None:
            return # Cannot load if file is not open

        while self._current_group_idx < len(self.group_names):
            group_name = self.group_names[self._current_group_idx]
            try:
                if group_name in self._file:
                    group = self._file[group_name]
                    if OBS_KEY in group and ACTION_BITS_KEY in group:
                        obs_group_data = group[OBS_KEY][:]
                        actions_bits_group_data = group[ACTION_BITS_KEY][:]

                        # Basic Validation
                        if obs_group_data.shape[0] > 0 \
                                and obs_group_data.shape[0] == actions_bits_group_data.shape[0] \
                                and obs_group_data.shape[1] == INPUT_DIM \
                                and actions_bits_group_data.shape[1] == CARD_BITS:
                            self._current_obs_data = obs_group_data
                            self._current_actions_bits_data = actions_bits_group_data
                            #logging.debug(f"Loaded data from group '{group_name}' with {self._current_obs_data.shape[0]} samples.")
                            return # Successfully loaded data for the current group

                        else:
                             logging.warning(f"Skipping group '{group_name}' due to shape/size mismatch or emptiness. Obs shape: {obs_group_data.shape}, Action bits shape: {actions_bits_group_data.shape}")
                    else:
                         logging.warning(f"Skipping group '{group_name}': Missing dataset(s) '{OBS_KEY}' or '{ACTION_BITS_KEY}'.")
                else:
                     logging.warning(f"Skipping group '{group_name}': Group not found in HDF5 file.")


            except Exception as e:
                logging.error(f"Error loading group '{group_name}': {e}")
                # traceback.print_exc() # Uncomment for detailed error during loading
            finally:
                # Move to the next group regardless of success/failure loading this one
                self._current_group_idx += 1


        # If loop finishes without returning, it means no more valid groups
        logging.info("Finished checking all specified groups.")
        self._current_obs_data = None # Indicate no more data

    def close(self):
        """Closes the HDF5 file handle."""
        if self._file is not None:
            try:
                self._file.close()
                logging.info("HDF5 file handle closed.")
            except Exception as e:
                 logging.error(f"Error closing HDF5 file: {e}")
            finally:
                 self._file = None # Ensure file is None after trying to close


    def __del__(self):
        """Ensure the file handle is closed when the iterator object is deleted."""
        # Be careful with __del__ and resource management, explicit close() is better
        # but this adds a layer of safety. Check if _file is not None to avoid errors
        if hasattr(self, '_file'): # Check if _file attribute exists
            self.close()


# --- Main Execution ---
if __name__ == "__main__":
    logging.info("--- Starting Card Policy Training (BC) with Memory Efficiency ---")

    # --- Get Group Names and Split ---
    all_group_names = []
    if not os.path.exists(HDF5_FILE_PATH):
        logging.critical(f"HDF5 file not found at: {HDF5_FILE_PATH}. Exiting.")
        exit()

    try:
        # Open and close just to get group keys
        with h5py.File(HDF5_FILE_PATH, 'r') as f:
            all_group_names = list(f.keys())
        logging.info(f"Found {len(all_group_names)} total game groups in HDF5 file.")
    except Exception as e:
        logging.critical(f"Failed to read group names from HDF5 file '{HDF5_FILE_PATH}': {e}. Exiting.")
        traceback.print_exc()
        exit()

    if not all_group_names:
        logging.critical("No game groups found in the HDF5 file. Exiting.")
        exit()

    logging.info(f"Splitting group names for BC (keeping {1-VALIDATION_SPLIT_SIZE:.0%} for training)...")
    train_group_names, val_group_names = train_test_split(
        all_group_names,
        test_size=VALIDATION_SPLIT_SIZE,
        random_state=RANDOM_SEED,
        shuffle=True
    )
    logging.info(f"BC Training groups: {len(train_group_names)}, Validation groups: {len(val_group_names)}")
    del all_group_names # Free memory from full list of names

    if not train_group_names:
        logging.critical("No training groups selected after split. Cannot train. Exiting.")
        exit()

    # --- Count Total Valid Training Samples ---
    # We need this count to set total_timesteps for BC training epochs.
    # This requires one pass through the training data using the iterator,
    # but without storing the data in memory.
    logging.info("Counting total valid training samples (this might take some time)...")
    total_training_samples = 0
    try:
        # Create a temporary iterator instance just for counting
        count_iterator = HDF5CardDemoIterator(HDF5_FILE_PATH, train_group_names)
        # Explicitly call __iter__ to ensure the file is opened
        count_iterator_instance = iter(count_iterator)
        
        while True:
            try:
                # Use next() on the iterator instance to pull samples
                next(count_iterator_instance)
                total_training_samples += 1
                if (total_training_samples) % 100000 == 0:
                     logging.info(f"Counted {total_training_samples} valid samples so far...")
            except StopIteration:
                break # Iterator is exhausted
            except Exception as e:
                logging.warning(f"Error while counting sample: {e}. Skipping.")
                # Optionally log more details if needed
                # traceback.print_exc()
                continue # Try to continue counting


    except Exception as e:
        logging.critical(f"Fatal error during valid sample counting: {e}. Exiting.")
        traceback.print_exc()
        # Ensure the file handle is closed even on error during counting loop setup
        if 'count_iterator' in locals():
             count_iterator.close()
        exit()
    finally:
         # Ensure the file handle is closed after counting
         if 'count_iterator' in locals():
             count_iterator.close()


    logging.info(f"Finished counting. Total valid training samples found: {total_training_samples}.")

    if total_training_samples == 0:
        logging.critical("No valid training samples found after filtering. Cannot train. Exiting.")
        exit()

    # --- Prepare Data & Spaces for Imitation Library ---
    # Create the *actual* iterator instance that BC will use during training
    train_demonstrations_iterator = HDF5CardDemoIterator(HDF5_FILE_PATH, train_group_names)

    observation_space = gym.spaces.Box(low=0, high=1, shape=(INPUT_DIM,), dtype=np.float32)
    action_space = gym.spaces.Discrete(CARD_ACTION_DIM) # Keep Discrete(9) for action index


    # --- Train Behavioral Cloning Model ---
    rng = np.random.default_rng(RANDOM_SEED)

    bc_trainer = bc.BC(
        observation_space=observation_space,
        action_space=action_space,
        demonstrations=train_demonstrations_iterator, # Pass the iterator directly
        batch_size=BC_BATCH_SIZE,
        optimizer_kwargs=dict(lr=BC_LEARNING_RATE),
        device=device,
        rng=rng,
        # Use default loss_calculator - should be fine with PyTorch 2+ and recent imitation/SB3
    )

    # Calculate total timesteps based on epochs and total valid samples
    total_timesteps = total_training_samples * BC_N_EPOCHS
    logging.info(f"Starting BC training for {BC_N_EPOCHS} epochs (~{total_training_samples} samples/epoch), totaling {total_timesteps} timesteps...")

    try:
        # Imitation's BC.train automatically creates a DataLoader from the iterator
        # and pulls total_timesteps samples.
        # Set log_interval based on total_timesteps for reasonable logging frequency
        log_interval = max(1, total_timesteps // 500) # Log ~500 times during training
        bc_trainer.train(total_timesteps=total_timesteps, log_interval=log_interval)
        logging.info("BC training completed.")
    except Exception as e:
        logging.critical(f"Error during BC training: {e}")
        traceback.print_exc()
    finally:
        # Ensure the main training iterator's file handle is closed
        # This is important if training is interrupted
        train_demonstrations_iterator.close()


    # --- Save Policy ---
    # Only attempt to save if training was attempted (total_timesteps > 0)
    if total_timesteps > 0:
        try:
            # The policy object is stored in bc_trainer.policy
            # Save it using the underlying SB3 policy save method
            # This saves an SB3-compatible policy, usually as a .zip file
            save_path = f"{POLICY_SAVE_PATH}.zip"
            bc_trainer.policy.save(save_path)
            logging.info(f"Card prediction policy saved successfully to {save_path}")
        except Exception as e:
            logging.error(f"Error saving BC policy: {e}")
            traceback.print_exc()
    else:
        logging.warning("Skipping policy save because no training was performed.")


    logging.info("--- Card Policy Training Finished ---")